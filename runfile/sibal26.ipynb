{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1101d4bd-a27d-49a7-bb7b-5cf2783e141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cd4bc9b289438e96eea10f6f9a6ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: fuel_term=-1.00, distance_term=-0.29, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.45\n",
      "Step 20: fuel_term=-1.00, distance_term=0.00, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.16\n",
      "Step 30: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.00, goal_reward=0.00, total_reward=-1.00\n",
      "Step 40: fuel_term=-1.00, distance_term=-0.31, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.48\n",
      "Step 50: fuel_term=-1.00, distance_term=-0.53, turn_penalty=0.67, goal_reward=0.00, total_reward=-2.20\n",
      "Step 60: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.95\n",
      "Step 70: fuel_term=-0.98, distance_term=-1.38, turn_penalty=1.34, goal_reward=0.00, total_reward=-3.70\n",
      "Step 80: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.67\n",
      "Step 90: fuel_term=-1.00, distance_term=0.86, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.09\n",
      "Step 100: fuel_term=-1.00, distance_term=0.86, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.09\n",
      "Step 110: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.95\n",
      "Step 120: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.05, goal_reward=0.00, total_reward=-1.05\n",
      "Step 130: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.95\n",
      "Step 140: fuel_term=-1.01, distance_term=0.50, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.46\n",
      "Step 150: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.67\n",
      "Step 160: fuel_term=-1.00, distance_term=0.86, turn_penalty=1.16, goal_reward=0.00, total_reward=-1.30\n",
      "Step 170: fuel_term=-1.00, distance_term=-0.34, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.50\n",
      "Step 180: fuel_term=-1.00, distance_term=0.51, turn_penalty=1.16, goal_reward=0.00, total_reward=-1.65\n",
      "Step 190: fuel_term=-1.00, distance_term=0.50, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.17\n",
      "Step 200: fuel_term=-1.00, distance_term=0.49, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.19\n",
      "Step 210: fuel_term=-1.00, distance_term=-0.49, turn_penalty=0.67, goal_reward=0.00, total_reward=-2.16\n",
      "Step 220: fuel_term=-1.00, distance_term=-0.37, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.54\n",
      "Step 230: fuel_term=-1.00, distance_term=-0.41, turn_penalty=1.34, goal_reward=0.00, total_reward=-2.76\n",
      "Step 240: fuel_term=-1.00, distance_term=-0.40, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.56\n",
      "Step 250: fuel_term=-1.00, distance_term=-1.35, turn_penalty=0.03, goal_reward=0.00, total_reward=-2.38\n",
      "Step 260: fuel_term=-1.00, distance_term=0.44, turn_penalty=1.34, goal_reward=0.00, total_reward=-1.90\n",
      "Step 270: fuel_term=-1.00, distance_term=-0.47, turn_penalty=0.05, goal_reward=0.00, total_reward=-1.53\n",
      "Step 280: fuel_term=-1.00, distance_term=-0.49, turn_penalty=0.95, goal_reward=0.00, total_reward=-2.43\n",
      "Step 290: fuel_term=-1.00, distance_term=0.91, turn_penalty=0.03, goal_reward=0.00, total_reward=-0.12\n",
      "Step 300: fuel_term=-1.00, distance_term=-0.43, turn_penalty=0.95, goal_reward=0.00, total_reward=-2.37\n",
      "Episode 0, Total Reward: 3041170.2525485973, Path Length: 300\n",
      "Step 10: fuel_term=-1.00, distance_term=-1.38, turn_penalty=0.95, goal_reward=0.00, total_reward=-3.33\n",
      "Step 20: fuel_term=-1.00, distance_term=-0.26, turn_penalty=1.34, goal_reward=0.00, total_reward=-2.61\n",
      "Step 30: fuel_term=-1.00, distance_term=0.81, turn_penalty=1.34, goal_reward=0.00, total_reward=-1.53\n",
      "Step 40: fuel_term=-1.00, distance_term=-1.40, turn_penalty=1.34, goal_reward=0.00, total_reward=-3.74\n",
      "Step 50: fuel_term=-1.00, distance_term=0.81, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.14\n",
      "Step 60: fuel_term=-1.00, distance_term=-1.39, turn_penalty=0.67, goal_reward=0.00, total_reward=-3.06\n",
      "Step 70: fuel_term=-1.00, distance_term=0.82, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.13\n",
      "Step 80: fuel_term=-1.00, distance_term=-0.59, turn_penalty=0.95, goal_reward=0.00, total_reward=-2.54\n",
      "Step 90: fuel_term=-1.00, distance_term=0.81, turn_penalty=0.67, goal_reward=0.00, total_reward=-0.86\n",
      "Step 100: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.67\n",
      "Step 110: fuel_term=-1.00, distance_term=-0.59, turn_penalty=0.95, goal_reward=0.00, total_reward=-2.54\n",
      "Step 120: fuel_term=-1.00, distance_term=1.40, turn_penalty=1.34, goal_reward=0.00, total_reward=-0.94\n",
      "Step 130: fuel_term=-1.00, distance_term=-0.59, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.75\n",
      "Step 140: fuel_term=-1.00, distance_term=-0.22, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.39\n",
      "Step 150: fuel_term=-1.00, distance_term=0.00, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.16\n",
      "Step 160: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.67\n",
      "Step 170: fuel_term=-1.00, distance_term=0.00, turn_penalty=1.34, goal_reward=0.00, total_reward=-2.34\n",
      "Step 180: fuel_term=-1.00, distance_term=1.39, turn_penalty=0.67, goal_reward=0.00, total_reward=-0.28\n",
      "Step 190: fuel_term=-1.00, distance_term=0.82, turn_penalty=0.04, goal_reward=0.00, total_reward=-0.21\n",
      "Step 200: fuel_term=-1.00, distance_term=1.39, turn_penalty=1.16, goal_reward=0.00, total_reward=-0.77\n",
      "Step 210: fuel_term=-1.00, distance_term=-0.25, turn_penalty=0.06, goal_reward=0.00, total_reward=-1.30\n",
      "Step 220: fuel_term=-1.00, distance_term=0.82, turn_penalty=1.16, goal_reward=0.00, total_reward=-1.34\n",
      "Step 230: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.67\n",
      "Step 240: fuel_term=-1.00, distance_term=1.39, turn_penalty=1.16, goal_reward=0.00, total_reward=-0.77\n",
      "Step 250: fuel_term=-1.00, distance_term=0.57, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.11\n",
      "Step 260: fuel_term=-1.00, distance_term=-0.28, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.44\n",
      "Step 270: fuel_term=-1.00, distance_term=-0.30, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.97\n",
      "Step 280: fuel_term=-1.00, distance_term=0.31, turn_penalty=1.34, goal_reward=0.00, total_reward=-2.03\n",
      "Step 290: fuel_term=-1.00, distance_term=-0.54, turn_penalty=1.34, goal_reward=0.00, total_reward=-2.88\n",
      "Step 300: fuel_term=9571.69, distance_term=0.54, turn_penalty=1.16, goal_reward=0.00, total_reward=9571.07\n",
      "Step 10: fuel_term=-1.00, distance_term=-0.29, turn_penalty=1.16, goal_reward=0.00, total_reward=-2.45\n",
      "Step 20: fuel_term=-1.00, distance_term=-1.39, turn_penalty=1.16, goal_reward=0.00, total_reward=-3.55\n",
      "Step 30: fuel_term=-1.00, distance_term=0.83, turn_penalty=1.16, goal_reward=0.00, total_reward=-1.34\n",
      "Step 40: fuel_term=-1.00, distance_term=0.82, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.13\n",
      "Step 50: fuel_term=-1.00, distance_term=-0.57, turn_penalty=0.67, goal_reward=0.00, total_reward=-2.24\n",
      "Step 60: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.95, goal_reward=0.00, total_reward=-1.95\n",
      "Step 70: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.67, goal_reward=0.00, total_reward=-1.67\n",
      "Step 80: fuel_term=-1.00, distance_term=0.00, turn_penalty=0.00, goal_reward=0.00, total_reward=-1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 561\u001b[0m\n\u001b[0;32m    559\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m    560\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim, action_dim)\n\u001b[1;32m--> 561\u001b[0m rewards, path_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n\u001b[0;32m    563\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Rewards Over Episodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 514\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(env, agent, max_episodes)\u001b[0m\n\u001b[0;32m    512\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(agent\u001b[38;5;241m.\u001b[39mepsilon_end, epsilon)\n\u001b[0;32m    513\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state, epsilon)\n\u001b[1;32m--> 514\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m q_values \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy_net(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_mode:\n",
      "Cell \u001b[1;32mIn[1], line 304\u001b[0m, in \u001b[0;36mNavigationEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_time \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time \u001b[38;5;241m=\u001b[39m next_time\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_tidal_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_tidal_to_grid()\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_wind_data()\n",
      "Cell \u001b[1;32mIn[1], line 241\u001b[0m, in \u001b[0;36mNavigationEnv.load_tidal_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_tidal_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    240\u001b[0m     time_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m     tidal_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtidal_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtidal_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtidal_data \u001b[38;5;241m=\u001b[39m tidal_data \u001b[38;5;28;01mif\u001b[39;00m tidal_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 216\u001b[0m, in \u001b[0;36mNavigationEnv.load_data\u001b[1;34m(self, data_dir, filename_prefix, time_str)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[time_str]\n\u001b[0;32m    215\u001b[0m data_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtime_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Data file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Episode will be terminated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import jit, float64, int64, boolean\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# CUDA 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 경험 저장을 위한 named tuple 정의\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# Dueling DQN 네트워크 정의\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.value_stream = nn.Linear(64, 1)\n",
    "        self.advantage_stream = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# SumTree 클래스 (PER 우선순위 관리)\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, priority)\n",
    "        self.write = (self.write + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, idx, priority):\n",
    "        change = priority - self.tree[idx]\n",
    "        self.tree[idx] = priority\n",
    "        while idx != 0:\n",
    "            idx = (idx - 1) // 2\n",
    "            self.tree[idx] += change\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = 0\n",
    "        while True:\n",
    "            left = 2 * idx + 1\n",
    "            right = left + 1\n",
    "            if left >= len(self.tree):\n",
    "                break\n",
    "            if s <= self.tree[left]:\n",
    "                idx = left\n",
    "            else:\n",
    "                s -= self.tree[left]\n",
    "                idx = right\n",
    "        data_idx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[data_idx]\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "# Numba로 최적화된 연료 소비 계산 함수 (기존 유지)\n",
    "@jit(nopython=True)\n",
    "def calculate_fuel_consumption(abs_action_angle, position, tidal_grid_dir, tidal_grid_speed, tidal_grid_valid, \n",
    "                               wind_grid_dir, wind_grid_speed, wind_grid_valid, k_c, k_w, n_rows, n_cols, \n",
    "                               V_s=6.68, f_0=1.0):\n",
    "    # 위치에서 행과 열 추출\n",
    "    row, col = position\n",
    "    \n",
    "    # 조류 방향과 속도 초기화\n",
    "    tidal_dir, tidal_speed = 0.0, 0.0\n",
    "    if 0 <= row < n_rows and 0 <= col < n_cols and tidal_grid_valid[row, col]:\n",
    "        tidal_dir = tidal_grid_dir[row, col]\n",
    "        tidal_speed = tidal_grid_speed[row, col]\n",
    "    \n",
    "    # 바람 방향과 속도 초기화\n",
    "    wind_dir, wind_speed = 0.0, 0.0\n",
    "    if 0 <= row < n_rows and 0 <= col < n_cols and wind_grid_valid[row, col]:\n",
    "        wind_dir = wind_grid_dir[row, col]\n",
    "        wind_speed = wind_grid_speed[row, col]\n",
    "    \n",
    "    # 각도를 라디안으로 변환\n",
    "    tidal_dir_rad = (90.0 - tidal_dir) * np.pi / 180.0\n",
    "    wind_dir_rad = (90.0 - wind_dir) * np.pi / 180.0\n",
    "    action_angle_rad = (90.0 - abs_action_angle) * np.pi / 180.0\n",
    "    \n",
    "    # 조류와 바람의 상대 각도 계산\n",
    "    theta_c = action_angle_rad - tidal_dir_rad\n",
    "    theta_w = action_angle_rad - wind_dir_rad\n",
    "    \n",
    "    # 조류 효과 계산: 제안서 공식 반영\n",
    "    tidal_effect = (V_s - tidal_speed * np.cos(theta_c)) / V_s  # 상대 속도 비율\n",
    "    tidal_effect = f_0 * (tidal_effect ** 3)  # 3제곱 후 f_0 곱하기\n",
    "    \n",
    "    # 바람 효과 계산: 바람 속도 제곱 반영\n",
    "    wind_effect = k_w * (wind_speed ** 2) * np.cos(theta_w)\n",
    "    \n",
    "    # 총 연료 소비: 조류 효과 + 바람 효과\n",
    "    total_fuel = tidal_effect + wind_effect\n",
    "    return total_fuel\n",
    "\n",
    "# Numba로 최적화된 데이터 매핑 함수 (기존 유지)\n",
    "@jit(nopython=True)\n",
    "def map_data_to_grid(positions, directions, speeds, lat_min, lat_max, lon_min, lon_max, n_rows, n_cols):\n",
    "    grid_dir = np.zeros((n_rows, n_cols), dtype=np.float64)\n",
    "    grid_speed = np.zeros((n_rows, n_cols), dtype=np.float64)\n",
    "    grid_valid = np.zeros((n_rows, n_cols), dtype=np.bool_)\n",
    "    \n",
    "    for i in range(len(positions)):\n",
    "        lat, lon = positions[i]\n",
    "        row = int((lat_max - lat) / (lat_max - lat_min) * n_rows)\n",
    "        col = int((lon - lon_min) / (lon_max - lon_min) * n_cols)\n",
    "        if 0 <= row < n_rows and 0 <= col < n_cols:\n",
    "            grid_dir[row, col] = directions[i]\n",
    "            grid_speed[row, col] = speeds[i]\n",
    "            grid_valid[row, col] = True\n",
    "    return grid_dir, grid_speed, grid_valid\n",
    "\n",
    "# 항해 환경 클래스 (기존 유지)\n",
    "class NavigationEnv:\n",
    "    def __init__(self):\n",
    "        self.grid = np.load('land_sea_grid_cartopy_downsized.npy')\n",
    "        self.n_rows, self.n_cols = self.grid.shape\n",
    "        self.lat_min, self.lat_max = 30.0, 38.0\n",
    "        self.lon_min, self.lon_max = 120.0, 127.0\n",
    "        self.start_pos = self.latlon_to_grid(37.46036, 126.52360)\n",
    "        self.end_pos = self.latlon_to_grid(30.62828, 122.06400)\n",
    "        self.step_time_minutes = 12\n",
    "        self.max_steps = 300\n",
    "        self.cumulative_time = 0\n",
    "        self.step_count = 0\n",
    "        self.tidal_data_dir = r\"C:\\baramproject\\tidal_database\"\n",
    "        self.wind_data_dir = r\"C:\\baramproject\\wind_database_2\"\n",
    "        self.action_space = np.array([0, 45, 90, 135, 180, -135, -90, -45], dtype=np.float64)\n",
    "        self.grid_directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.k_c = 0.1\n",
    "        self.k_w = 0.005\n",
    "        self.path = []\n",
    "        self.tidal_cache = {}\n",
    "        self.wind_cache = {}\n",
    "        self.reset()\n",
    "\n",
    "    def latlon_to_grid(self, lat, lon):\n",
    "        row = int((self.lat_max - lat) / (self.lat_max - self.lat_min) * self.n_rows)\n",
    "        col = int((lon - self.lon_min) / (self.lon_max - self.lon_min) * self.n_cols) \n",
    "        return row, col\n",
    "\n",
    "    def reset(self, start_time=None):\n",
    "        start_date = datetime(2018, 1, 1, 0, 0)\n",
    "        end_date = datetime(2018, 12, 29, 0, 0)\n",
    "        if start_time is None:\n",
    "            time_delta = (end_date - start_date).total_seconds()\n",
    "            random_seconds = np.random.randint(0, int(time_delta / 60 / 30) + 1) * 30 * 60\n",
    "            start_time = start_date + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        self.current_pos = self.start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.current_time = start_time\n",
    "        self.cumulative_time = 0\n",
    "        self.load_tidal_data()\n",
    "        self.map_tidal_to_grid()\n",
    "        self.load_wind_data()\n",
    "        self.map_wind_to_grid()\n",
    "        self.prev_distance = self.get_distance_to_end()\n",
    "        self.step_count = 0\n",
    "        self.path = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def get_relative_position_and_angle(self):\n",
    "        rel_pos = np.array(self.end_pos, dtype=np.float64) - np.array(self.current_pos, dtype=np.float64)\n",
    "        distance = np.linalg.norm(rel_pos)\n",
    "        end_angle = (np.degrees(np.arctan2(rel_pos[1], -rel_pos[0])) % 360.0)\n",
    "        return rel_pos, distance, end_angle\n",
    "\n",
    "    def get_distance_to_end(self):\n",
    "        rel_pos = np.array(self.end_pos, dtype=np.float64) - np.array(self.current_pos, dtype=np.float64)\n",
    "        return np.linalg.norm(rel_pos)\n",
    "\n",
    "    def angle_to_grid_direction(self, abs_action_angle):\n",
    "        grid_angles = np.array([0, 45, 90, 135, 180, 225, 270, 315], dtype=np.float64)\n",
    "        angle_diff = np.abs(grid_angles - abs_action_angle)\n",
    "        closest_idx = np.argmin(angle_diff)\n",
    "        return self.grid_directions[closest_idx]\n",
    "\n",
    "    def load_data(self, data_dir, filename_prefix, time_str):\n",
    "        if filename_prefix == \"tidal_\":\n",
    "            cache = self.tidal_cache\n",
    "        elif filename_prefix == \"wind_\":\n",
    "            cache = self.wind_cache\n",
    "        else:\n",
    "            raise ValueError(\"Invalid filename_prefix\")\n",
    "    \n",
    "        if time_str in cache:\n",
    "            return cache[time_str]\n",
    "        \n",
    "        data_file = os.path.join(data_dir, f\"{filename_prefix}{time_str}.json\")\n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Warning: Data file {data_file} not found. Episode will be terminated.\")\n",
    "            return None\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        result = data[\"result\"][\"data\"]\n",
    "        \n",
    "        cache[time_str] = result\n",
    "        return result\n",
    "\n",
    "    def map_tidal_to_grid(self):\n",
    "        if self.tidal_data is not None:\n",
    "            positions = np.array([(float(item[\"pre_lat\"]), float(item[\"pre_lon\"])) for item in self.tidal_data], dtype=np.float64)\n",
    "            directions = np.array([float(item[\"current_dir\"]) for item in self.tidal_data], dtype=np.float64)\n",
    "            speeds = np.array([float(item[\"current_speed\"]) for item in self.tidal_data], dtype=np.float64)\n",
    "            self.tidal_grid_dir, self.tidal_grid_speed, self.tidal_grid_valid = map_data_to_grid(\n",
    "                positions, directions, speeds, self.lat_min, self.lat_max, self.lon_min, self.lon_max, self.n_rows, self.n_cols\n",
    "            )\n",
    "        else:\n",
    "            self.tidal_grid_dir = np.zeros((self.n_rows, self.n_cols), dtype=np.float64)\n",
    "            self.tidal_grid_speed = np.zeros((self.n_rows, self.n_cols), dtype=np.float64)\n",
    "            self.tidal_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=np.bool_)\n",
    "\n",
    "    def load_tidal_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        tidal_data = self.load_data(self.tidal_data_dir, \"tidal_\", time_str)\n",
    "        self.tidal_data = tidal_data if tidal_data is not None else None\n",
    "\n",
    "    def map_wind_to_grid(self):\n",
    "        if self.wind_data is not None:\n",
    "            positions = np.array([(float(item[\"pre_lat\"]), float(item[\"pre_lon\"])) for item in self.wind_data], dtype=np.float64)\n",
    "            directions = np.array([float(item[\"wind_dir\"]) for item in self.wind_data], dtype=np.float64)\n",
    "            speeds = np.array([float(item[\"wind_speed\"]) for item in self.wind_data], dtype=np.float64)\n",
    "            self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid = map_data_to_grid(\n",
    "                positions, directions, speeds, self.lat_min, self.lat_max, self.lon_min, self.lon_max, self.n_rows, self.n_cols\n",
    "            )\n",
    "        else:\n",
    "            self.wind_grid_dir = np.zeros((self.n_rows, self.n_cols), dtype=np.float64)\n",
    "            self.wind_grid_speed = np.zeros((self.n_rows, self.n_cols), dtype=np.float64)\n",
    "            self.wind_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=np.bool_)\n",
    "\n",
    "    def load_wind_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        wind_data = self.load_data(self.wind_data_dir, \"wind_\", time_str)\n",
    "        self.wind_data = wind_data if wind_data is not None else None\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        rel_action_angle = self.action_space[action]\n",
    "        abs_action_angle = (end_angle + rel_action_angle) % 360.0\n",
    "        \n",
    "        # 방향 전환 각도 계산\n",
    "        turn_penalty = 0.0\n",
    "        if hasattr(self, 'previous_direction') and self.previous_direction is not None:\n",
    "            angle_diff = min((abs_action_angle - self.previous_direction) % 360.0, \n",
    "                             (self.previous_direction - abs_action_angle) % 360.0)\n",
    "        else:\n",
    "            angle_diff = 0.0\n",
    "        \n",
    "        move_dir = self.angle_to_grid_direction(abs_action_angle)\n",
    "        new_pos = (self.current_pos[0] + move_dir[0], self.current_pos[1] + move_dir[1])\n",
    "        \n",
    "        # 연료 소비량 계산\n",
    "        current_fuel = calculate_fuel_consumption(abs_action_angle, self.current_pos, self.tidal_grid_dir, \n",
    "                                                  self.tidal_grid_speed, self.tidal_grid_valid, \n",
    "                                                  self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid, \n",
    "                                                  self.k_c, self.k_w, self.n_rows, self.n_cols)\n",
    "        next_fuel = calculate_fuel_consumption(abs_action_angle, new_pos, self.tidal_grid_dir, \n",
    "                                               self.tidal_grid_speed, self.tidal_grid_valid, \n",
    "                                               self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid, \n",
    "                                               self.k_c, self.k_w, self.n_rows, self.n_cols)\n",
    "        \n",
    "        # 위치 업데이트\n",
    "        if (0 <= new_pos[0] < self.n_rows and 0 <= new_pos[1] < self.n_cols and \n",
    "            self.grid[new_pos[0], new_pos[1]] == 0):\n",
    "            self.current_pos = new_pos\n",
    "            self.path.append(self.current_pos)\n",
    "        self.previous_direction = abs_action_angle\n",
    "        self.prev_action = action\n",
    "        \n",
    "        # 시간 업데이트\n",
    "        self.cumulative_time += self.step_time_minutes\n",
    "        if self.cumulative_time >= 30:\n",
    "            next_time = self.current_time + timedelta(minutes=30)\n",
    "            end_date = datetime(2018, 12, 31, 23, 30)\n",
    "            if next_time <= end_date:\n",
    "                self.current_time = next_time\n",
    "                self.load_tidal_data()\n",
    "                self.map_tidal_to_grid()\n",
    "                self.load_wind_data()\n",
    "                self.map_wind_to_grid()\n",
    "            else:\n",
    "                print(\"Warning: Time exceeds 2018 range. Keeping previous data.\")\n",
    "            self.cumulative_time -= 30\n",
    "        \n",
    "        # 상태 및 거리 계산\n",
    "        state = self._get_state()\n",
    "        d_t = self.get_distance_to_end()\n",
    "        distance_reduction = self.prev_distance - d_t\n",
    "        \n",
    "        # 초기값 설정\n",
    "        if not hasattr(self, 'prev_fuel'):\n",
    "            self.prev_fuel = current_fuel  # 첫 스텝에서 초기화\n",
    "        if not hasattr(self, 'd_init'):\n",
    "            self.d_init = self.get_distance_to_end()  # 초기 거리 설정\n",
    "        \n",
    "        # 가중치 및 기준값 설정\n",
    "        w_f = 1.0  # 연료 소비 가중치\n",
    "        w_d = 1.0  # 거리 감소 가중치 (선형으로 변경했으므로 약간 크게 설정 가능)\n",
    "        w_t = 0.1  # 방향 전환 페널티 가중치\n",
    "        f_0 = 1.0  # 기준 연료 소비량\n",
    "        distance_reduction = self.prev_distance - d_t  # 거리 감소량\n",
    "    \n",
    "        # 보상 계산\n",
    "        fuel_term = -w_f * current_fuel * (1 + (self.prev_fuel - current_fuel) / f_0)\n",
    "        distance_term = w_d * distance_reduction  # 지수 함수 대신 선형 함수 사용\n",
    "        turn_penalty = w_t * (angle_diff ** 0.5)\n",
    "        goal_reward = 100.0 if d_t == 0 else 0.0\n",
    "        \n",
    "        reward = fuel_term + distance_term - turn_penalty + goal_reward\n",
    "    \n",
    "        # 디버깅용 로깅 (10스텝마다 출력)\n",
    "        if self.step_count % 10 == 0:\n",
    "            print(f\"Step {self.step_count}: fuel_term={fuel_term:.10f}, distance_term={distance_term:.10f}, turn_penalty={turn_penalty:.10f}, goal_reward={goal_reward:.10f}, total_reward={reward:.10f}\")\n",
    "\n",
    "        \n",
    "        # 이전 값 업데이트\n",
    "        self.prev_fuel = current_fuel\n",
    "        self.prev_distance = d_t\n",
    "        \n",
    "        # 종료 조건\n",
    "        done = tuple(self.current_pos) == self.end_pos or self.step_count >= self.max_steps\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        row, col = self.current_pos\n",
    "        rel_pos, _, end_angle = self.get_relative_position_and_angle()\n",
    "        tidal_dir, tidal_speed = 0.0, 0.0\n",
    "        if hasattr(self, 'tidal_grid_valid') and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0.0, 0.0\n",
    "        if hasattr(self, 'wind_grid_valid') and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        return np.array([rel_pos[0], rel_pos[1], end_angle, tidal_dir, tidal_speed, wind_dir, wind_speed], dtype=np.float64)\n",
    "\n",
    "# DQN 에이전트 클래스 (PER + n_steps=3 복구)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.buffer_size = 100000\n",
    "        self.target_update = 1000\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 10000\n",
    "        self.n_steps = 3\n",
    "        self.alpha = 0.6\n",
    "        self.beta_start = 0.4\n",
    "        self.beta_end = 1.0\n",
    "        self.beta_steps = 50000\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        self.memory = SumTree(self.buffer_size)\n",
    "        self.step_count = 0\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        self.step_count += 1\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        max_priority = np.max(self.memory.tree[-self.memory.capacity:]) if self.memory.n_entries > 0 else 1.0\n",
    "        self.memory.add(max_priority, experience)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_batch(self, beta):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        priorities = []\n",
    "        segment = self.memory.total() / self.batch_size\n",
    "        for i in range(self.batch_size):\n",
    "            a, b = segment * i, segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.memory.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = np.array(priorities) / self.memory.total()\n",
    "        weights = (self.memory.n_entries * sampling_probabilities) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return batch, idxs, torch.FloatTensor(weights).to(device)\n",
    "\n",
    "    def compute_n_step_target(self, batch, idxs):\n",
    "        batch_size = len(batch)\n",
    "        states = torch.FloatTensor(np.array([e.state for e in batch])).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in batch]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in batch]).to(device)\n",
    "        next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(device)\n",
    "        dones = torch.FloatTensor([e.done for e in batch]).to(device)\n",
    "    \n",
    "        # n-step 타겟 계산\n",
    "        targets = rewards.clone()\n",
    "        for i in range(1, self.n_steps):\n",
    "            future_idxs = [idx + i for idx in idxs if idx + i < self.memory_counter]\n",
    "            if future_idxs:\n",
    "                future_batch = []\n",
    "                valid_future_idxs = []\n",
    "                for idx in future_idxs:\n",
    "                    if idx < self.memory.n_entries + self.memory.capacity - 1:\n",
    "                        future_batch.append(self.memory.data[idx % self.memory.capacity])\n",
    "                        valid_future_idxs.append(idx)\n",
    "                \n",
    "                if len(future_batch) > 0:\n",
    "                    future_rewards = torch.FloatTensor([e.reward for e in future_batch]).to(device)\n",
    "                    future_dones = torch.FloatTensor([e.done for e in future_batch]).to(device)\n",
    "                    \n",
    "                    # mask 생성: valid_future_idxs에 해당하는 위치만 True\n",
    "                    mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "                    mask[:len(valid_future_idxs)] = True\n",
    "                    \n",
    "                    # 오른쪽 항을 mask에 맞춰 슬라이싱\n",
    "                    targets[mask] += (self.gamma ** i) * future_rewards[:len(valid_future_idxs)] * (1 - future_dones[:len(valid_future_idxs)])\n",
    "    \n",
    "        # 마지막 n-step 후 Q값 추가\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).argmax(1)\n",
    "            target_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            targets += (self.gamma ** self.n_steps) * target_q * (1 - dones)\n",
    "    \n",
    "        return states, actions, targets\n",
    "\n",
    "    def compute_loss(self, batch, idxs, weights):\n",
    "        if not batch:\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        states, actions, targets = self.compute_n_step_target(batch, idxs)\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # TD 오차 계산 및 우선순위 업데이트\n",
    "        td_errors = (q_values.detach() - targets).cpu().numpy()\n",
    "        loss = (weights * (q_values - targets) ** 2).mean()\n",
    "\n",
    "        # 메모리 우선순위 업데이트\n",
    "        for idx, td_error in zip(idxs, td_errors):\n",
    "            priority = (abs(td_error) + 0.01) ** self.alpha\n",
    "            self.memory.update(idx, priority)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        if self.memory.n_entries < self.batch_size + self.n_steps - 1:\n",
    "            return\n",
    "        beta = self.beta_start + (self.beta_end - self.beta_start) * min(1.0, self.step_count / self.beta_steps)\n",
    "        batch, idxs, weights = self.sample_batch(beta)\n",
    "        loss = self.compute_loss(batch, idxs, weights)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# 학습 루프 정의 (기존 유지)\n",
    "def train_dqn(env, agent, max_episodes=5000):\n",
    "    rewards = []\n",
    "    path_lengths = []\n",
    "    epsilon = agent.epsilon_start\n",
    "    image_dir = r\"C:\\baramproject\\trained_model\\sibal26\\episode_debug_image\"\n",
    "    data_dir = r\"C:\\baramproject\\trained_model\\sibal26\\episode_debug_data\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for episode in tqdm(range(max_episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path_length = 0\n",
    "        done = False\n",
    "        debug_mode = episode % 100 == 0\n",
    "        debug_data = [] if debug_mode else None\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = agent.epsilon_start *(agent.epsilon_end / agent.epsilon_start) ** (agent.step_count / agent.epsilon_decay)\n",
    "            epsilon = max(agent.epsilon_end, epsilon)\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            q_values = agent.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\n",
    "            if debug_mode:\n",
    "                debug_data.append({\n",
    "                    \"step\": path_length,\n",
    "                    \"state\": state.tolist(),\n",
    "                    \"action\": action,\n",
    "                    \"reward\": reward,\n",
    "                    \"next_state\": next_state.tolist(),\n",
    "                    \"q_values\": q_values.tolist(),\n",
    "                    \"epsilon\": epsilon\n",
    "                })\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            if path_length % 4 == 0:\n",
    "                agent.update()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path_length += 1\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        path_lengths.append(path_length)\n",
    "\n",
    "        if debug_mode:\n",
    "            with open(os.path.join(data_dir, f\"episode_{episode}.json\"), 'w') as f:\n",
    "                json.dump(debug_data, f, indent=4)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Path Length: {path_length}\")\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(env.grid, cmap='gray')\n",
    "            path_array = np.array(env.path)\n",
    "            plt.plot(path_array[:, 1], path_array[:, 0], 'r-', label='Path')\n",
    "            plt.plot(env.start_pos[1], env.start_pos[0], 'go', label='Start') \n",
    "            plt.plot(env.end_pos[1], env.end_pos[0], 'bo', label='End')\n",
    "            plt.legend()\n",
    "            plt.title(f\"Episode {episode} Path\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), r\"C:\\baramproject\\trained_model\\sibal26\\navigation_model.pth\")\n",
    "    return rewards, path_lengths\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    env = NavigationEnv()\n",
    "    state_dim = 7\n",
    "    action_dim = len(env.action_space)\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    rewards, path_lengths = train_dqn(env, agent)\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Total Rewards Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    plt.plot(path_lengths)\n",
    "    plt.title(\"Path Lengths Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Path Length\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
