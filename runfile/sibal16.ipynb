{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9d9507-424b-4f39-bd6c-b20a08ab17f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n",
      "모델 파일 'C:\\baramproject\\trained_model\\sibal16\\navigation_model.pth'가 없습니다. 새로 학습을 시작합니다.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefd9565996146fda5ccd91ccc05bd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "학습 진행률:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_100.png\n",
      "Episode 200 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_200.png\n",
      "Episode 300 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_300.png\n",
      "Episode 400 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_400.png\n",
      "Episode 500 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_500.png\n",
      "Episode 600 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_600.png\n",
      "Episode 700 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_700.png\n",
      "Episode 800 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_800.png\n",
      "Episode 900 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_900.png\n",
      "Episode 1000 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1000.png\n",
      "Episode 1100 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1100.png\n",
      "Episode 1200 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1200.png\n",
      "Episode 1300 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1300.png\n",
      "Episode 1400 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1400.png\n",
      "Episode 1500 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1500.png\n",
      "Episode 1600 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1600.png\n",
      "Episode 1700 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1700.png\n",
      "Episode 1800 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1800.png\n",
      "Episode 1900 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_1900.png\n",
      "Episode 2000 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2000.png\n",
      "Episode 2100 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2100.png\n",
      "Episode 2200 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2200.png\n",
      "Episode 2300 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2300.png\n",
      "Episode 2400 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2400.png\n",
      "Episode 2500 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2500.png\n",
      "Episode 2600 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2600.png\n",
      "Episode 2700 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2700.png\n",
      "Episode 2800 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2800.png\n",
      "Episode 2900 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_2900.png\n",
      "Episode 3000 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3000.png\n",
      "Episode 3100 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3100.png\n",
      "Episode 3200 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3200.png\n",
      "Episode 3300 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3300.png\n",
      "Episode 3400 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3400.png\n",
      "Episode 3500 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3500.png\n",
      "Episode 3600 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3600.png\n",
      "Episode 3700 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3700.png\n",
      "Episode 3800 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3800.png\n",
      "Episode 3900 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_3900.png\n",
      "Episode 4000 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4000.png\n",
      "Episode 4100 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4100.png\n",
      "Episode 4200 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4200.png\n",
      "Episode 4300 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4300.png\n",
      "Episode 4400 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4400.png\n",
      "Episode 4500 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4500.png\n",
      "Episode 4600 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4600.png\n",
      "Episode 4700 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4700.png\n",
      "Episode 4800 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4800.png\n",
      "Episode 4900 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_4900.png\n",
      "Episode 5000 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5000.png\n",
      "Episode 5100 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5100.png\n",
      "Episode 5200 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5200.png\n",
      "Episode 5300 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5300.png\n",
      "Episode 5400 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5400.png\n",
      "Episode 5500 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5500.png\n",
      "Episode 5600 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5600.png\n",
      "Episode 5700 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5700.png\n",
      "Episode 5800 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5800.png\n",
      "Episode 5900 - 중간 경로 시각화 (Start: (18, 219), End: (248, 69))\n",
      "이미지 저장: C:\\baramproject\\trained_model\\sibal16\\episode_debug\\episode2_5900.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khg95\\AppData\\Local\\Temp\\ipykernel_8392\\4150612122.py:238: RuntimeWarning: invalid value encountered in divide\n",
      "  probs /= probs.sum()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 362\u001b[0m\n\u001b[0;32m    359\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    361\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, BETA_START \u001b[38;5;241m+\u001b[39m steps \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m BETA_START) \u001b[38;5;241m/\u001b[39m BETA_FRAMES)\n\u001b[1;32m--> 362\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m TARGET_UPDATE \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    365\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_target()\n",
      "Cell \u001b[1;32mIn[14], line 275\u001b[0m, in \u001b[0;36mRainbowAgent.optimize_model\u001b[1;34m(self, beta)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m<\u001b[39m BATCH_SIZE:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m samples, indices, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msamples)\n\u001b[0;32m    277\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(states)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 239\u001b[0m, in \u001b[0;36mPrioritizedReplay.sample\u001b[1;34m(self, batch_size, beta)\u001b[0m\n\u001b[0;32m    237\u001b[0m probs \u001b[38;5;241m=\u001b[39m priorities \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m    238\u001b[0m probs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m--> 239\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[0;32m    241\u001b[0m weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m*\u001b[39m probs[indices]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mbeta)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:994\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import amp\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# CUDA 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 하이퍼파라미터 설정 (Rainbow DQN)\n",
    "GAMMA = 0.99\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 256\n",
    "MEMORY_SIZE = 10000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 500\n",
    "TARGET_UPDATE = 10\n",
    "ALPHA = 0.6  # Prioritized Replay의 alpha\n",
    "BETA_START = 0.4\n",
    "BETA_FRAMES = 1000\n",
    "tidal_data_cache = {}\n",
    "\n",
    "# 1. 그리드 맵 로드\n",
    "grid = np.load(r'C:/baramproject/sibal/land_sea_grid_cartopy_downsized.npy')\n",
    "n_rows, n_cols = grid.shape  # 270x236\n",
    "\n",
    "# 2. 좌표 변환 함수\n",
    "def latlon_to_grid(lat, lon, lat_min=30, lat_max=38, lon_min=120, lon_max=127):\n",
    "    row = int((lat_max - lat) / (lat_max - lat_min) * n_rows)\n",
    "    col = int((lon - lon_min) / (lon_max - lon_min) * n_cols)\n",
    "    return min(max(row, 0), n_rows-1), min(max(col, 0), n_cols-1)\n",
    "\n",
    "# 시작점과 도착점 설정\n",
    "start_lat, start_lon = 37.46036, 126.52360  # 인천항\n",
    "end_lat, end_lon = 30.62828, 122.06400     # 상하이항\n",
    "start_pos = latlon_to_grid(start_lat, start_lon)\n",
    "end_pos = latlon_to_grid(end_lat, end_lon)\n",
    "\n",
    "# 3. 유클리드 거리 계산 함수\n",
    "def euclidean_distance(pos1, pos2):\n",
    "    return np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)\n",
    "\n",
    "# 4. 조류 데이터 로드 함수\n",
    "def load_tidal_data(time_str):\n",
    "    if time_str in tidal_data_cache:\n",
    "        return tidal_data_cache[time_str]\n",
    "    file_path = f\"C:/baramproject/tidal_database/tidal_{time_str}.json\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"파일 {file_path}가 존재하지 않습니다.\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    tidal_data_cache[time_str] = data['result']['data']\n",
    "    return tidal_data_cache[time_str]\n",
    "\n",
    "# 5. 조류 데이터를 그리드에 매핑\n",
    "def map_tidal_to_grid(tidal_data, n_rows, n_cols):\n",
    "    current_grid = np.zeros((n_rows, n_cols, 2))  # [방향, 속도]\n",
    "    for entry in tidal_data:\n",
    "        lat = float(entry['pre_lat'])\n",
    "        lon = float(entry['pre_lon'])\n",
    "        row, col = latlon_to_grid(lat, lon)\n",
    "        if 0 <= row < n_rows and 0 <= col < n_cols and grid[row, col] == 0:\n",
    "            direction = float(entry['current_dir'])\n",
    "            speed = float(entry['current_speed'])\n",
    "            current_grid[row, col] = [direction, speed]\n",
    "    return current_grid\n",
    "\n",
    "# 6. 환경 클래스 정의\n",
    "class NavigationEnv:\n",
    "    def __init__(self, grid, start_pos, end_pos, tidal_database_path, max_steps=300, step_time_minutes=12):\n",
    "        self.grid = grid\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        self.current_pos = start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.tidal_database_path = tidal_database_path\n",
    "        self.max_steps = max_steps\n",
    "        self.step_time_minutes = step_time_minutes\n",
    "        self.current_time = None\n",
    "        self.current_tidal_data = None\n",
    "        self.cumulative_time = 0\n",
    "\n",
    "    def reset(self, start_time=None):\n",
    "        if start_time is None:\n",
    "            start_time = self._random_start_time()\n",
    "        self.current_pos = self.start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.current_time = start_time\n",
    "        self.cumulative_time = 0\n",
    "        tidal_data = load_tidal_data(self.current_time.strftime(\"%Y%m%d_%H%M\"))\n",
    "        self.current_tidal_data = map_tidal_to_grid(tidal_data, n_rows, n_cols)\n",
    "        return self.get_state()\n",
    "\n",
    "    def _random_start_time(self):\n",
    "        start_datetime = datetime(2018, 1, 1, 0, 0)\n",
    "        end_datetime = datetime(2018, 12, 28, 0, 0)\n",
    "        total_minutes = int((end_datetime - start_datetime).total_seconds() / 60)\n",
    "        random_minutes = random.randint(0, total_minutes // 30) * 30\n",
    "        return start_datetime + timedelta(minutes=random_minutes)\n",
    "\n",
    "    def get_state(self):\n",
    "        rel_row = (self.current_pos[0] - self.start_pos[0]) / n_rows\n",
    "        rel_col = (self.current_pos[1] - self.start_pos[1]) / n_cols\n",
    "        dist_to_end = euclidean_distance(self.current_pos, self.end_pos) / max(n_rows, n_cols)\n",
    "        dx = self.end_pos[0] - self.current_pos[0]\n",
    "        dy = self.end_pos[1] - self.current_pos[1]\n",
    "        angle_to_goal = math.atan2(dy, dx) / math.pi\n",
    "\n",
    "        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "        tidal_dirs = []\n",
    "        tidal_speeds = []\n",
    "        for d in directions:\n",
    "            r = self.current_pos[0] + d[0]\n",
    "            c = self.current_pos[1] + d[1]\n",
    "            if 0 <= r < n_rows and 0 <= c < n_cols:\n",
    "                tidal_dirs.append(self.current_tidal_data[r, c, 0] / 360.0)\n",
    "                tidal_speeds.append(self.current_tidal_data[r, c, 1] / 10.0)\n",
    "            else:\n",
    "                tidal_dirs.append(0.0)\n",
    "                tidal_speeds.append(0.0)\n",
    "\n",
    "        return np.concatenate(([rel_row, rel_col, dist_to_end, angle_to_goal], tidal_dirs, tidal_speeds))\n",
    "\n",
    "    def get_action_deltas(self):\n",
    "        dx = self.end_pos[0] - self.current_pos[0]\n",
    "        dy = self.end_pos[1] - self.current_pos[1]\n",
    "        angle_to_goal = math.atan2(dy, dx)\n",
    "        angles = [0, math.pi/4, -math.pi/4, math.pi/2, -math.pi/2, 3*math.pi/4, -3*math.pi/4, math.pi]\n",
    "        action_deltas = []\n",
    "        for angle_offset in angles:\n",
    "            target_angle = angle_to_goal + angle_offset\n",
    "            delta_row = round(-math.cos(target_angle))\n",
    "            delta_col = round(math.sin(target_angle))\n",
    "            action_deltas.append((delta_row, delta_col))\n",
    "        return action_deltas\n",
    "\n",
    "    def step(self, action):\n",
    "        action_deltas = self.get_action_deltas()\n",
    "        move = action_deltas[action]\n",
    "        new_pos = (self.current_pos[0] + move[0], self.current_pos[1] + move[1])\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if (0 <= new_pos[0] < n_rows and 0 <= new_pos[1] < n_cols and self.grid[new_pos] == 0):\n",
    "            prev_dist = euclidean_distance(self.current_pos, self.end_pos)\n",
    "            self.current_pos = new_pos\n",
    "            new_dist = euclidean_distance(self.current_pos, self.end_pos)\n",
    "\n",
    "            dist_change = prev_dist - new_dist\n",
    "            reward += dist_change * 50\n",
    "            reward -= 1\n",
    "\n",
    "            row, col = self.current_pos\n",
    "            current_dir = self.current_tidal_data[row, col, 0] * math.pi / 180\n",
    "            current_speed = self.current_tidal_data[row, col, 1]\n",
    "            move_angle = math.atan2(move[1], move[0]) if move != (0, 0) else 0\n",
    "            angle_diff = abs((current_dir - move_angle + math.pi) % (2 * math.pi) - math.pi)\n",
    "            if angle_diff < math.pi / 4:\n",
    "                reward += current_speed * 2\n",
    "            elif angle_diff > 3 * math.pi / 4:\n",
    "                reward -= current_speed * 1.5\n",
    "\n",
    "            if new_dist < 5:\n",
    "                reward += 200 * (1 - new_dist / 5)\n",
    "            if new_dist < 1:\n",
    "                reward += 1000\n",
    "                done = True\n",
    "\n",
    "            pos_tuple = tuple(self.current_pos)\n",
    "            self.visit_count[pos_tuple] = self.visit_count.get(pos_tuple, 0) + 1\n",
    "            if self.visit_count[pos_tuple] > 1:\n",
    "                reward -= 10 * self.visit_count[pos_tuple]\n",
    "\n",
    "            self.prev_action = move\n",
    "            self.cumulative_time += self.step_time_minutes\n",
    "            if self.cumulative_time >= 30:\n",
    "                self.current_time += timedelta(minutes=30)\n",
    "                tidal_data = load_tidal_data(self.current_time.strftime(\"%Y%m%d_%H%M\"))\n",
    "                self.current_tidal_data = map_tidal_to_grid(tidal_data, n_rows, n_cols)\n",
    "                self.cumulative_time -= 30\n",
    "        else:\n",
    "            reward = -50\n",
    "            done = False\n",
    "\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "# 7. Rainbow DQN 모델 정의 (Dueling 구조 사용)\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim=20, output_dim=8):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value = nn.Linear(64, 1)\n",
    "        self.advantage = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value(x)\n",
    "        advantage = self.advantage(x)\n",
    "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "\n",
    "# 8. Prioritized Replay Memory (Rainbow DQN의 PER)\n",
    "class PrioritizedReplay:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.memory = []\n",
    "        self.priorities = []\n",
    "        self.pos = 0\n",
    "\n",
    "    def push(self, error, sample):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(sample)\n",
    "            self.priorities.append(error)\n",
    "        else:\n",
    "            self.memory[self.pos] = sample\n",
    "            self.priorities[self.pos] = error\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.memory), BATCH_SIZE, p=probs)\n",
    "        samples = [self.memory[idx] for idx in indices]\n",
    "        weights = (len(self.memory) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            self.priorities[idx] = error\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# 9. Rainbow DQN 에이전트\n",
    "class RainbowAgent:\n",
    "    def __init__(self, state_dim=20, action_dim=8):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.memory = PrioritizedReplay(MEMORY_SIZE, ALPHA)\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n",
    "        self.steps = 0\n",
    "        self.scaler = amp.GradScaler('cuda')\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).squeeze(0).argmax().item()\n",
    "\n",
    "    def optimize_model(self, beta):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        samples, indices, weights = self.memory.sample(BATCH_SIZE, beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        states = torch.stack(states).to(device).squeeze(1)\n",
    "        actions = torch.LongTensor(actions).to(device).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device).squeeze(1)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        with amp.autocast('cuda'):  # Mixed Precision 연산 시작\n",
    "            q_values = self.policy_net(states).gather(1, actions).squeeze(1)\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            targets = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "            loss = (weights * (q_values - targets) ** 2).mean()\n",
    "\n",
    "        self.scaler.scale(loss).backward()  # 손실 스케일링 후 역전파\n",
    "        self.scaler.step(self.optimizer)    # 옵티마이저 업데이트\n",
    "        self.scaler.update()                # 스케일러 업데이트\n",
    "\n",
    "        errors = (q_values - targets).abs().detach().cpu().numpy()\n",
    "        self.memory.update_priorities(indices, errors)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# 10. 학습 설정\n",
    "tidal_database_path = r\"C:\\baramproject\\tidal_database\"\n",
    "env = NavigationEnv(grid, start_pos, end_pos, tidal_database_path, max_steps=300)\n",
    "agent = RainbowAgent(state_dim=20, action_dim=8)\n",
    "\n",
    "# 모델 로드 또는 새로 초기화\n",
    "model_path = r'C:\\baramproject\\trained_model\\sibal16\\navigation_model.pth'\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        agent.policy_net.load_state_dict(torch.load(model_path))\n",
    "        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        print(f\"기존 모델 '{model_path}'를 불러왔습니다. 재학습을 시작합니다.\")\n",
    "        epsilon = 0.3\n",
    "        num_episodes = 300\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}. 새로 학습을 시작합니다.\")\n",
    "        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        epsilon = 1.0\n",
    "        num_episodes = 2000\n",
    "else:\n",
    "    print(f\"모델 파일 '{model_path}'가 없습니다. 새로 학습을 시작합니다.\")\n",
    "    agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "    epsilon = EPSILON_START\n",
    "    num_episodes = 20000\n",
    "\n",
    "# 이미지 저장 디렉토리 설정\n",
    "save_dir = r'C:\\baramproject\\trained_model\\sibal16\\episode_debug'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    print(f\"디렉토리 '{save_dir}'를 생성했습니다.\")\n",
    "\n",
    "# 11. 학습 루프\n",
    "rewards = []\n",
    "path_lengths = []\n",
    "progress_bar = tqdm(range(num_episodes), desc=\"학습 진행률\")\n",
    "beta = BETA_START\n",
    "\n",
    "for episode in progress_bar:\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    path = [env.current_pos]\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < env.max_steps:\n",
    "        action = agent.select_action(state, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = torch.FloatTensor(next_state).to(device).unsqueeze(0)\n",
    "        total_reward += reward\n",
    "        path.append(env.current_pos)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_value = agent.policy_net(state)[0, action]\n",
    "            next_q_value = agent.target_net(next_state).max(dim=1)[0]\n",
    "            target = reward + GAMMA * next_q_value * (1 - done)\n",
    "            error = abs(q_value - target).item()\n",
    "        agent.memory.push(error, (state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        beta = min(1.0, BETA_START + steps * (1.0 - BETA_START) / BETA_FRAMES)\n",
    "        agent.optimize_model(beta)\n",
    "\n",
    "        if steps % TARGET_UPDATE == 0:\n",
    "            agent.update_target()\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon - (EPSILON_START - EPSILON_END) / EPSILON_DECAY)\n",
    "    rewards.append(total_reward)\n",
    "    path_lengths.append(len(path))\n",
    "    progress_bar.set_postfix({'Reward': total_reward, 'Path Length': len(path)})\n",
    "\n",
    "    if episode % 100 == 0 and episode > 0:\n",
    "        print(f\"Episode {episode} - 중간 경로 시각화 (Start: {start_pos}, End: {end_pos})\")\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(grid, cmap='gray', origin='upper')\n",
    "        path_rows = [pos[0] for pos in path]\n",
    "        path_cols = [pos[1] for pos in path]\n",
    "        plt.plot(path_cols, path_rows, 'r-', linewidth=2)\n",
    "        plt.plot(start_pos[1], start_pos[0], 'go')\n",
    "        plt.plot(end_pos[1], end_pos[0], 'bo')\n",
    "        plt.title(f\"Path Visualization at Episode {episode}\")\n",
    "        save_path = os.path.join(save_dir, f\"episode2_{episode}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"이미지 저장: {save_path}\")\n",
    "        plt.close()\n",
    "\n",
    "# 12. 모델 저장\n",
    "torch.save(agent.policy_net.state_dict(), r'C:\\baramproject\\trained_model\\sibal16\\navigation_model.pth')\n",
    "print(\"모델이 'navigation_model.pth' 파일로 저장되었습니다.\")\n",
    "\n",
    "# 13. 학습 결과 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, label='Total Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Learning Reward Graph')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(path_lengths, label='Path Length')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Path Length')\n",
    "plt.title('Learning Path Length Graph')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 14. 최적 경로 시각화 함수\n",
    "def plot_path(path, start_pos, end_pos, grid):\n",
    "    path_rows = [pos[0] for pos in path]\n",
    "    path_cols = [pos[1] for pos in path]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(grid, cmap='gray', origin='upper')\n",
    "    plt.plot(path_cols, path_rows, 'r-', linewidth=2, label='Optimal Path')\n",
    "    plt.plot(start_pos[1], start_pos[0], 'go', label=f'Start {start_pos}')\n",
    "    plt.plot(end_pos[1], end_pos[0], 'bo', label=f'End {end_pos}')\n",
    "    lon_ticks = np.linspace(0, n_cols, 8)\n",
    "    lon_labels = np.linspace(120, 127, 8)\n",
    "    plt.xticks(lon_ticks, [f\"{lon:.1f}°E\" for lon in lon_labels])\n",
    "    lat_ticks = np.linspace(0, n_rows, 9)\n",
    "    lat_labels = np.linspace(38, 30, 9)\n",
    "    plt.yticks(lat_ticks, [f\"{lat:.1f}°N\" for lat in lat_labels])\n",
    "    plt.xlabel('Longitude Grid')\n",
    "    plt.ylabel('Latitude Grid')\n",
    "    plt.title('Optimal Navigation Path')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 최적 경로 시각화\n",
    "plot_path(path, start_pos, end_pos, grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
