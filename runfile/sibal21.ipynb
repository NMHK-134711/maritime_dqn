{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566e8ddb-d391-4526-ad0d-3dbf7d430721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b647b8d74a8845c2a217f2bbe683fa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -2932.9631011240285, Path Length: 300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 485\u001b[0m\n\u001b[0;32m    482\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m    483\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim, action_dim)\n\u001b[1;32m--> 485\u001b[0m rewards, path_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n\u001b[0;32m    488\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Rewards Over Episodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 441\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(env, agent, max_episodes)\u001b[0m\n\u001b[0;32m    430\u001b[0m debug_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: path_length,\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: state\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m: epsilon\n\u001b[0;32m    438\u001b[0m })\n\u001b[0;32m    440\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_experience(state, action, reward, next_state, done)\n\u001b[1;32m--> 441\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    444\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[5], line 400\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# 모델 업데이트\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 400\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\baramproject\\sibal\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\baramproject\\sibal\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\baramproject\\sibal\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#초기모델 학습코드\n",
    "import numpy as np\n",
    "from math import atan2, degrees, radians, cos, sin\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# CUDA 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 경험 저장을 위한 named tuple 정의\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# Dueling DQN 네트워크 정의\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # 상태 가치 스트림\n",
    "        self.value_stream = nn.Linear(64, 1)\n",
    "        # 액션 이점 스트림\n",
    "        self.advantage_stream = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Q 값 계산: V(s) + (A(s,a) - mean(A(s)))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# SumTree 클래스 정의 (PER을 위한 우선순위 관리)\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # 우선순위를 저장하는 트리\n",
    "        self.data = np.zeros(capacity, dtype=object)  # 경험 데이터를 저장\n",
    "        self.data_pointer = 0  # 다음 저장 위치\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0  # 버퍼가 가득 차면 처음부터 덮어씀\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        # 부모 노드 업데이트\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, s):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if s <= self.tree[left_child_idx]:\n",
    "                parent_idx = left_child_idx\n",
    "            else:\n",
    "                s -= self.tree[left_child_idx]\n",
    "                parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]  # 루트 노드의 합계\n",
    "\n",
    "# 항해 환경 클래스 정의 (변경 없음)\n",
    "class NavigationEnv:\n",
    "    def __init__(self):\n",
    "        self.grid = np.load('land_sea_grid_cartopy_downsized.npy')\n",
    "        self.n_rows, self.n_cols = self.grid.shape\n",
    "        self.lat_min, self.lat_max = 30, 38\n",
    "        self.lon_min, self.lon_max = 120, 127\n",
    "        self.start_pos = self.latlon_to_grid(37.46036, 126.52360)\n",
    "        self.end_pos = self.latlon_to_grid(30.62828, 122.06400)\n",
    "        self.step_time_minutes = 12\n",
    "        self.max_steps = 300\n",
    "        self.cumulative_time = 0\n",
    "        self.step_count = 0\n",
    "        self.tidal_data_dir = r\"C:\\baramproject\\tidal_database\"\n",
    "        self.wind_data_dir = r\"C:\\baramproject\\wind_database_2\"\n",
    "        self.action_space = np.array([0, 45, 90, 135, 180, -135, -90, -45])\n",
    "        self.grid_directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.k_c = 0.1\n",
    "        self.k_w = 0.005\n",
    "        self.path = []\n",
    "        self.reset()\n",
    "\n",
    "    def latlon_to_grid(self, lat, lon):\n",
    "        row = int((self.lat_max - lat) / (self.lat_max - self.lat_min) * self.n_rows)\n",
    "        col = int((lon - self.lon_min) / (self.lon_max - self.lon_min) * self.n_cols)\n",
    "        return row, col\n",
    "\n",
    "    def reset(self, start_time=None):\n",
    "        start_date = datetime(2018, 1, 1, 0, 0)\n",
    "        end_date = datetime(2018, 12, 27, 0, 0)\n",
    "        if start_time is None:\n",
    "            time_delta = (end_date - start_date).total_seconds()\n",
    "            random_seconds = np.random.randint(0, int(time_delta / 60 / 30) + 1) * 30 * 60\n",
    "            start_time = start_date + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        self.current_pos = self.start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.current_time = start_time\n",
    "        self.cumulative_time = 0\n",
    "        self.load_tidal_data()\n",
    "        self.map_tidal_to_grid()\n",
    "        self.load_wind_data()\n",
    "        self.map_wind_to_grid()\n",
    "        self.prev_distance = self.get_distance_to_end()\n",
    "        self.step_count = 0\n",
    "        self.path = [self.current_pos]\n",
    "        return self._get_state()\n",
    "\n",
    "    def get_relative_position_and_angle(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        distance = np.linalg.norm(rel_pos)\n",
    "        end_angle = degrees(atan2(rel_pos[1], rel_pos[0])) % 360\n",
    "        return rel_pos, distance, end_angle\n",
    "\n",
    "    def get_distance_to_end(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        return np.linalg.norm(rel_pos)\n",
    "\n",
    "    def angle_to_grid_direction(self, abs_action_angle):\n",
    "        grid_angles = np.array([0, 45, 90, 135, 180, 225, 270, 315])\n",
    "        angle_diff = np.abs(grid_angles - abs_action_angle)\n",
    "        closest_idx = np.argmin(angle_diff)\n",
    "        return self.grid_directions[closest_idx]\n",
    "\n",
    "    def load_data(self, data_dir, filename_prefix, time_str):\n",
    "        data_file = os.path.join(data_dir, f\"{filename_prefix}{time_str}.json\")\n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Warning: Data file {data_file} not found. Episode will be terminated.\")\n",
    "            return None\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data[\"result\"][\"data\"]\n",
    "\n",
    "    def map_data_to_grid(self, data, dir_key, speed_key):\n",
    "        grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "        if data is None:\n",
    "            return grid_dir, grid_speed, grid_valid\n",
    "        positions = [(float(item[\"pre_lat\"]), float(item[\"pre_lon\"])) for item in data]\n",
    "        directions = [float(item[dir_key]) for item in data]\n",
    "        speeds = [float(item[speed_key]) for item in data]\n",
    "        for pos, dir, speed in zip(positions, directions, speeds):\n",
    "            lat, lon = pos\n",
    "            row, col = self.latlon_to_grid(lat, lon)\n",
    "            if 0 <= row < self.n_rows and 0 <= col < self.n_cols:\n",
    "                grid_dir[row, col] = dir\n",
    "                grid_speed[row, col] = speed\n",
    "                grid_valid[row, col] = True\n",
    "        return grid_dir, grid_speed, grid_valid\n",
    "\n",
    "    def load_tidal_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        tidal_data = self.load_data(self.tidal_data_dir, \"tidal_\", time_str)\n",
    "        self.tidal_data = tidal_data if tidal_data is not None else None\n",
    "\n",
    "    def map_tidal_to_grid(self):\n",
    "        if self.tidal_data is not None:\n",
    "            self.tidal_grid_dir, self.tidal_grid_speed, self.tidal_grid_valid = self.map_data_to_grid(\n",
    "                self.tidal_data, \"current_dir\", \"current_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.tidal_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def load_wind_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        wind_data = self.load_data(self.wind_data_dir, \"wind_\", time_str)\n",
    "        self.wind_data = wind_data if wind_data is not None else None\n",
    "\n",
    "    def map_wind_to_grid(self):\n",
    "        if self.wind_data is not None:\n",
    "            self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid = self.map_data_to_grid(\n",
    "                self.wind_data, \"wind_dir\", \"wind_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.wind_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def calculate_fuel_consumption(self, abs_action_angle, position):\n",
    "        row, col = position\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        tidal_dir_rad = (90 - tidal_dir) * np.pi / 180\n",
    "        wind_dir_rad = (90 - wind_dir) * np.pi / 180\n",
    "        action_angle_rad = (90 - abs_action_angle) * np.pi / 180\n",
    "        theta_c = action_angle_rad - tidal_dir_rad\n",
    "        theta_w = action_angle_rad - wind_dir_rad\n",
    "        f_0 = 1\n",
    "        tidal_effect = -self.k_c * tidal_speed * cos(theta_c)\n",
    "        wind_effect = self.k_w * wind_speed * cos(theta_w)\n",
    "        total_fuel = f_0 + wind_effect + tidal_effect\n",
    "        return total_fuel\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        rel_action_angle = self.action_space[action]\n",
    "        abs_action_angle = (end_angle + rel_action_angle) % 360\n",
    "        turn_penalty = 0\n",
    "        if hasattr(self, 'previous_direction') and self.previous_direction is not None:\n",
    "            angle_diff = min((abs_action_angle - self.previous_direction) % 360, \n",
    "                             (self.previous_direction - abs_action_angle) % 360)\n",
    "            turn_penalty = angle_diff * 0.1\n",
    "        move_dir = self.angle_to_grid_direction(abs_action_angle)\n",
    "        new_pos = (self.current_pos[0] + move_dir[0], self.current_pos[1] + move_dir[1])\n",
    "        current_fuel = self.calculate_fuel_consumption(abs_action_angle, self.current_pos)\n",
    "        next_fuel = self.calculate_fuel_consumption(abs_action_angle, new_pos)\n",
    "        fuel_reduction = current_fuel - next_fuel\n",
    "        if (0 <= new_pos[0] < self.n_rows and 0 <= new_pos[1] < self.n_cols and \n",
    "            self.grid[new_pos[0], new_pos[1]] == 0):\n",
    "            self.current_pos = new_pos\n",
    "            self.path.append(self.current_pos)\n",
    "        self.previous_direction = abs_action_angle\n",
    "        self.prev_action = action\n",
    "        self.cumulative_time += self.step_time_minutes\n",
    "        if self.cumulative_time >= 30:\n",
    "            next_time = self.current_time + timedelta(minutes=30)\n",
    "            end_date = datetime(2018, 12, 31, 23, 30)\n",
    "            if next_time <= end_date:\n",
    "                self.current_time = next_time\n",
    "                self.load_tidal_data()\n",
    "                self.map_tidal_to_grid()\n",
    "                self.load_wind_data()\n",
    "                self.map_wind_to_grid()\n",
    "            else:\n",
    "                print(\"Warning: Time exceeds 2018 range. Keeping previous data.\")\n",
    "            self.cumulative_time -= 30\n",
    "        state = self._get_state()\n",
    "        current_distance = self.get_distance_to_end()\n",
    "        distance_reward = (self.prev_distance - current_distance) * 2.0\n",
    "        \n",
    "        self.prev_distance = current_distance\n",
    "        goal_reward = 100 if tuple(self.current_pos) == self.end_pos else 0\n",
    "        reward = -current_fuel + fuel_reduction * 1.0 + distance_reward - turn_penalty + goal_reward\n",
    "        done = tuple(self.current_pos) == self.end_pos or self.step_count >= self.max_steps\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        row, col = self.current_pos\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if hasattr(self, 'tidal_grid_valid') and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if hasattr(self, 'wind_grid_valid') and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        return np.array([rel_pos[0], rel_pos[1], distance, tidal_dir, tidal_speed, wind_dir, wind_speed])\n",
    "\n",
    "# DQN 에이전트 클래스 정의 (PER 통합)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # 기존 하이퍼파라미터\n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.buffer_size = 100000\n",
    "        self.target_update = 1000\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 10000\n",
    "        \n",
    "        # PER 관련 하이퍼파라미터\n",
    "        self.alpha = 0.6  # 우선순위 가중치 조절\n",
    "        self.beta_start = 0.4  # 초기 샘플링 편향 보정\n",
    "        self.beta_end = 1.0  # 최종 샘플링 편향 보정\n",
    "        \n",
    "        # 신경망 및 옵티마이저\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        \n",
    "        # PER을 위한 SumTree 메모리\n",
    "        self.memory = SumTree(self.buffer_size)\n",
    "        self.step_count = 0\n",
    "        self.max_priority = 1.0  # 초기 최대 우선순위\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        self.step_count += 1\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        # 새로운 경험을 최대 우선순위로 저장\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.add(self.max_priority, experience)\n",
    "\n",
    "    def sample_batch(self, beta):\n",
    "        # 우선순위에 비례하여 경험 샘플링\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        priorities = []\n",
    "        segment = self.memory.total_priority() / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.memory.get_leaf(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        \n",
    "        # 샘플링 확률과 가중치 계산\n",
    "        sampling_probabilities = np.array(priorities) / self.memory.total_priority()\n",
    "        is_weight = np.power(self.buffer_size * sampling_probabilities, -beta)\n",
    "        is_weight /= is_weight.max()  # 정규화\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def compute_loss(self, batch, idxs, is_weight, beta):\n",
    "        # 배치에서 데이터 추출\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        # Q 값 계산\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(dim=1)[0]\n",
    "            targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        # TD 오차 계산 및 우선순위 업데이트\n",
    "        td_errors = torch.abs(targets - q_values)\n",
    "        loss = (torch.FloatTensor(is_weight).to(device) * (q_values - targets.detach()) ** 2).mean()\n",
    "        \n",
    "        for idx, td_error in zip(idxs, td_errors.detach().cpu().numpy()):\n",
    "            priority = td_error.item() ** self.alpha\n",
    "            self.memory.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        if self.memory.data_pointer < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Beta 값을 학습 진행에 따라 증가\n",
    "        beta = self.beta_start + (self.beta_end - self.beta_start) * min(1.0, self.step_count / 50000)\n",
    "        batch, idxs, is_weight = self.sample_batch(beta)\n",
    "        loss = self.compute_loss(batch, idxs, is_weight, beta)\n",
    "        \n",
    "        # 모델 업데이트\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# 학습 루프 정의 (변경 없음)\n",
    "def train_dqn(env, agent, max_episodes=20000):\n",
    "    rewards = []\n",
    "    path_lengths = []\n",
    "    epsilon = agent.epsilon_start\n",
    "    \n",
    "    image_dir = r\"C:\\baramproject\\trained_model\\sibal21\\episode_debug_image\"\n",
    "    data_dir = r\"C:\\baramproject\\trained_model\\sibal21\\episode_debug_data\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for episode in tqdm(range(max_episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path_length = 0\n",
    "        done = False\n",
    "        debug_data = []\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = max(agent.epsilon_end, epsilon - (agent.epsilon_start - agent.epsilon_end) / agent.epsilon_decay)\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            q_values = agent.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\n",
    "            debug_data.append({\n",
    "                \"step\": path_length,\n",
    "                \"state\": state.tolist(),\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"next_state\": next_state.tolist(),\n",
    "                \"q_values\": q_values.tolist(),\n",
    "                \"epsilon\": epsilon\n",
    "            })\n",
    "            \n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path_length += 1\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        path_lengths.append(path_length)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Path Length: {path_length}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(env.grid, cmap='gray')\n",
    "            path_array = np.array(env.path)\n",
    "            plt.plot(path_array[:, 1], path_array[:, 0], 'r-', label='Path')\n",
    "            plt.plot(env.start_pos[1], env.start_pos[0], 'go', label='Start')\n",
    "            plt.plot(env.end_pos[1], env.end_pos[0], 'bo', label='End')\n",
    "            plt.legend()\n",
    "            plt.title(f\"Episode {episode} Path\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            with open(os.path.join(data_dir, f\"episode_{episode}.json\"), 'w') as f:\n",
    "                json.dump(debug_data, f, indent=4)\n",
    "        \n",
    "        if episode % 1000 == 0 and episode > 0:\n",
    "            plt.plot(rewards)\n",
    "            plt.title(\"Total Rewards Over Episodes\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"rewards_episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), r\"C:\\baramproject\\trained_model\\sibal21\\navigation_model.pth\")\n",
    "    return rewards, path_lengths\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    env = NavigationEnv()\n",
    "    state_dim = 7\n",
    "    action_dim = len(env.action_space)\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    rewards, path_lengths = train_dqn(env, agent)\n",
    "    \n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Total Rewards Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(path_lengths)\n",
    "    plt.title(\"Path Lengths Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Path Length\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78734ff8-8af9-4f1c-8a46-a32f5d79f1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "기존 모델을 C:\\baramproject\\trained_model\\sibal21\\navigation_model.pth에서 불러왔습니다. 추가 학습을 진행합니다.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f794661777c4a21af7d053220f3476c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -3923.787061758698, Path Length: 400\n",
      "Episode 100, Total Reward: -116.45858207707381, Path Length: 400\n",
      "Episode 200, Total Reward: 58.11937983724458, Path Length: 400\n",
      "Episode 300, Total Reward: 11.415534287131152, Path Length: 400\n",
      "Episode 400, Total Reward: -31.187600195375282, Path Length: 400\n",
      "Episode 500, Total Reward: -1.2001890329661649, Path Length: 400\n",
      "Episode 600, Total Reward: -120.10545946304612, Path Length: 400\n",
      "Episode 700, Total Reward: -36.38414994829327, Path Length: 400\n",
      "Episode 800, Total Reward: 1.8662044465155816, Path Length: 400\n",
      "Episode 900, Total Reward: -40.406117830586595, Path Length: 400\n",
      "Episode 1000, Total Reward: -66.29545474922337, Path Length: 400\n",
      "Episode 1100, Total Reward: 8.500221548641182, Path Length: 400\n",
      "Episode 1200, Total Reward: 169.9615367835946, Path Length: 400\n",
      "Episode 1300, Total Reward: 57.710840376161755, Path Length: 400\n",
      "Episode 1400, Total Reward: -72.86488394771892, Path Length: 400\n",
      "Episode 1500, Total Reward: 23.368127461933604, Path Length: 400\n",
      "Episode 1600, Total Reward: 60.522635289769866, Path Length: 400\n",
      "Episode 1700, Total Reward: 48.340191412722305, Path Length: 400\n",
      "Episode 1800, Total Reward: -99.42590225656903, Path Length: 400\n",
      "Episode 1900, Total Reward: -16.511744211813834, Path Length: 400\n",
      "Episode 2000, Total Reward: -5.5313241163845035, Path Length: 400\n"
     ]
    }
   ],
   "source": [
    "#기존모델 추가학습코드\n",
    "import numpy as np\n",
    "from math import atan2, degrees, radians, cos, sin\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# CUDA 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 경험 저장을 위한 named tuple 정의\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# Dueling DQN 네트워크 정의\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # 상태 가치 스트림\n",
    "        self.value_stream = nn.Linear(64, 1)\n",
    "        # 액션 이점 스트림\n",
    "        self.advantage_stream = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Q 값 계산: V(s) + (A(s,a) - mean(A(s)))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# SumTree 클래스 정의 (PER을 위한 우선순위 관리)\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # 우선순위를 저장하는 트리\n",
    "        self.data = np.zeros(capacity, dtype=object)  # 경험 데이터를 저장\n",
    "        self.data_pointer = 0  # 다음 저장 위치\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0  # 버퍼가 가득 차면 처음부터 덮어씀\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        # 부모 노드 업데이트\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, s):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if s <= self.tree[left_child_idx]:\n",
    "                parent_idx = left_child_idx\n",
    "            else:\n",
    "                s -= self.tree[left_child_idx]\n",
    "                parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]  # 루트 노드의 합계\n",
    "\n",
    "# 항해 환경 클래스 정의 (변경 없음)\n",
    "class NavigationEnv:\n",
    "    def __init__(self):\n",
    "        self.grid = np.load('land_sea_grid_cartopy_downsized.npy')\n",
    "        self.n_rows, self.n_cols = self.grid.shape\n",
    "        self.lat_min, self.lat_max = 30, 38\n",
    "        self.lon_min, self.lon_max = 120, 127\n",
    "        self.start_pos = self.latlon_to_grid(37.46036, 126.52360)\n",
    "        self.end_pos = self.latlon_to_grid(30.62828, 122.06400)\n",
    "        self.step_time_minutes = 12\n",
    "        self.max_steps = 400\n",
    "        self.cumulative_time = 0\n",
    "        self.step_count = 0\n",
    "        self.tidal_data_dir = r\"C:\\baramproject\\tidal_database\"\n",
    "        self.wind_data_dir = r\"C:\\baramproject\\wind_database_2\"\n",
    "        self.action_space = np.array([0, 45, 90, 135, 180, -135, -90, -45])\n",
    "        self.grid_directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.k_c = 0.1\n",
    "        self.k_w = 0.005\n",
    "        self.path = []\n",
    "        self.reset()\n",
    "\n",
    "    def latlon_to_grid(self, lat, lon):\n",
    "        row = int((self.lat_max - lat) / (self.lat_max - self.lat_min) * self.n_rows)\n",
    "        col = int((lon - self.lon_min) / (self.lon_max - self.lon_min) * self.n_cols)\n",
    "        return row, col\n",
    "\n",
    "    def reset(self, start_time=None):\n",
    "        start_date = datetime(2018, 1, 1, 0, 0)\n",
    "        end_date = datetime(2018, 12, 27, 0, 0)\n",
    "        if start_time is None:\n",
    "            time_delta = (end_date - start_date).total_seconds()\n",
    "            random_seconds = np.random.randint(0, int(time_delta / 60 / 30) + 1) * 30 * 60\n",
    "            start_time = start_date + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        self.current_pos = self.start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.current_time = start_time\n",
    "        self.cumulative_time = 0\n",
    "        self.load_tidal_data()\n",
    "        self.map_tidal_to_grid()\n",
    "        self.load_wind_data()\n",
    "        self.map_wind_to_grid()\n",
    "        self.prev_distance = self.get_distance_to_end()\n",
    "        self.step_count = 0\n",
    "        self.path = [self.current_pos]\n",
    "        return self._get_state()\n",
    "\n",
    "    def get_relative_position_and_angle(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        distance = np.linalg.norm(rel_pos)\n",
    "        end_angle = degrees(atan2(rel_pos[1], rel_pos[0])) % 360\n",
    "        return rel_pos, distance, end_angle\n",
    "\n",
    "    def get_distance_to_end(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        return np.linalg.norm(rel_pos)\n",
    "\n",
    "    def angle_to_grid_direction(self, abs_action_angle):\n",
    "        grid_angles = np.array([0, 45, 90, 135, 180, 225, 270, 315])\n",
    "        angle_diff = np.abs(grid_angles - abs_action_angle)\n",
    "        closest_idx = np.argmin(angle_diff)\n",
    "        return self.grid_directions[closest_idx]\n",
    "\n",
    "    def load_data(self, data_dir, filename_prefix, time_str):\n",
    "        data_file = os.path.join(data_dir, f\"{filename_prefix}{time_str}.json\")\n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Warning: Data file {data_file} not found. Episode will be terminated.\")\n",
    "            return None\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data[\"result\"][\"data\"]\n",
    "\n",
    "    def map_data_to_grid(self, data, dir_key, speed_key):\n",
    "        grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "        if data is None:\n",
    "            return grid_dir, grid_speed, grid_valid\n",
    "        positions = [(float(item[\"pre_lat\"]), float(item[\"pre_lon\"])) for item in data]\n",
    "        directions = [float(item[dir_key]) for item in data]\n",
    "        speeds = [float(item[speed_key]) for item in data]\n",
    "        for pos, dir, speed in zip(positions, directions, speeds):\n",
    "            lat, lon = pos\n",
    "            row, col = self.latlon_to_grid(lat, lon)\n",
    "            if 0 <= row < self.n_rows and 0 <= col < self.n_cols:\n",
    "                grid_dir[row, col] = dir\n",
    "                grid_speed[row, col] = speed\n",
    "                grid_valid[row, col] = True\n",
    "        return grid_dir, grid_speed, grid_valid\n",
    "\n",
    "    def load_tidal_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        tidal_data = self.load_data(self.tidal_data_dir, \"tidal_\", time_str)\n",
    "        self.tidal_data = tidal_data if tidal_data is not None else None\n",
    "\n",
    "    def map_tidal_to_grid(self):\n",
    "        if self.tidal_data is not None:\n",
    "            self.tidal_grid_dir, self.tidal_grid_speed, self.tidal_grid_valid = self.map_data_to_grid(\n",
    "                self.tidal_data, \"current_dir\", \"current_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.tidal_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def load_wind_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        wind_data = self.load_data(self.wind_data_dir, \"wind_\", time_str)\n",
    "        self.wind_data = wind_data if wind_data is not None else None\n",
    "\n",
    "    def map_wind_to_grid(self):\n",
    "        if self.wind_data is not None:\n",
    "            self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid = self.map_data_to_grid(\n",
    "                self.wind_data, \"wind_dir\", \"wind_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.wind_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def calculate_fuel_consumption(self, abs_action_angle, position):\n",
    "        row, col = position\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        tidal_dir_rad = (90 - tidal_dir) * np.pi / 180\n",
    "        wind_dir_rad = (90 - wind_dir) * np.pi / 180\n",
    "        action_angle_rad = (90 - abs_action_angle) * np.pi / 180\n",
    "        theta_c = action_angle_rad - tidal_dir_rad\n",
    "        theta_w = action_angle_rad - wind_dir_rad\n",
    "        f_0 = 1\n",
    "        tidal_effect = -self.k_c * tidal_speed * cos(theta_c)\n",
    "        wind_effect = self.k_w * wind_speed * cos(theta_w)\n",
    "        total_fuel = f_0 + wind_effect + tidal_effect\n",
    "        return total_fuel\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        rel_action_angle = self.action_space[action]\n",
    "        abs_action_angle = (end_angle + rel_action_angle) % 360\n",
    "        turn_penalty = 0\n",
    "        if hasattr(self, 'previous_direction') and self.previous_direction is not None:\n",
    "            angle_diff = min((abs_action_angle - self.previous_direction) % 360, \n",
    "                             (self.previous_direction - abs_action_angle) % 360)\n",
    "            turn_penalty = angle_diff * 0.1\n",
    "        move_dir = self.angle_to_grid_direction(abs_action_angle)\n",
    "        new_pos = (self.current_pos[0] + move_dir[0], self.current_pos[1] + move_dir[1])\n",
    "        current_fuel = self.calculate_fuel_consumption(abs_action_angle, self.current_pos)\n",
    "        next_fuel = self.calculate_fuel_consumption(abs_action_angle, new_pos)\n",
    "        fuel_reduction = current_fuel - next_fuel\n",
    "        if (0 <= new_pos[0] < self.n_rows and 0 <= new_pos[1] < self.n_cols and \n",
    "            self.grid[new_pos[0], new_pos[1]] == 0):\n",
    "            self.current_pos = new_pos\n",
    "            self.path.append(self.current_pos)\n",
    "        self.previous_direction = abs_action_angle\n",
    "        self.prev_action = action\n",
    "        self.cumulative_time += self.step_time_minutes\n",
    "        if self.cumulative_time >= 30:\n",
    "            next_time = self.current_time + timedelta(minutes=30)\n",
    "            end_date = datetime(2018, 12, 31, 23, 30)\n",
    "            if next_time <= end_date:\n",
    "                self.current_time = next_time\n",
    "                self.load_tidal_data()\n",
    "                self.map_tidal_to_grid()\n",
    "                self.load_wind_data()\n",
    "                self.map_wind_to_grid()\n",
    "            else:\n",
    "                print(\"Warning: Time exceeds 2018 range. Keeping previous data.\")\n",
    "            self.cumulative_time -= 30\n",
    "        state = self._get_state()\n",
    "        current_distance = self.get_distance_to_end()\n",
    "        distance_reward = (self.prev_distance - current_distance) * 2.0\n",
    "        \n",
    "        self.prev_distance = current_distance\n",
    "        goal_reward = 100 if tuple(self.current_pos) == self.end_pos else 0\n",
    "        reward = -current_fuel + fuel_reduction * 1.0 + distance_reward - turn_penalty + goal_reward\n",
    "        done = tuple(self.current_pos) == self.end_pos or self.step_count >= self.max_steps\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        row, col = self.current_pos\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if hasattr(self, 'tidal_grid_valid') and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if hasattr(self, 'wind_grid_valid') and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        return np.array([rel_pos[0], rel_pos[1], distance, tidal_dir, tidal_speed, wind_dir, wind_speed])\n",
    "\n",
    "# DQN 에이전트 클래스 정의 (PER 통합 및 모델 로드 추가)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, model_path=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # 기존 하이퍼파라미터\n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.buffer_size = 100000\n",
    "        self.target_update = 1000\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 10000\n",
    "        \n",
    "        # PER 관련 하이퍼파라미터\n",
    "        self.alpha = 0.6  # 우선순위 가중치 조절\n",
    "        self.beta_start = 0.4  # 초기 샘플링 편향 보정\n",
    "        self.beta_end = 1.0  # 최종 샘플링 편향 보정\n",
    "        \n",
    "        # 신경망 및 옵티마이저\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # 모델 파일이 존재하면 로드\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.policy_net.load_state_dict(torch.load(model_path))\n",
    "            print(f\"기존 모델을 {model_path}에서 불러왔습니다. 추가 학습을 진행합니다.\")\n",
    "        else:\n",
    "            print(\"모델 파일이 존재하지 않습니다. 새로운 모델로 학습을 시작합니다.\")\n",
    "        \n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        \n",
    "        # PER을 위한 SumTree 메모리\n",
    "        self.memory = SumTree(self.buffer_size)\n",
    "        self.step_count = 0\n",
    "        self.max_priority = 1.0  # 초기 최대 우선순위\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        self.step_count += 1\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        # 새로운 경험을 최대 우선순위로 저장\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.add(self.max_priority, experience)\n",
    "\n",
    "    def sample_batch(self, beta):\n",
    "        # 우선순위에 비례하여 경험 샘플링\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        priorities = []\n",
    "        segment = self.memory.total_priority() / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.memory.get_leaf(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        \n",
    "        # 샘플링 확률과 가중치 계산\n",
    "        sampling_probabilities = np.array(priorities) / self.memory.total_priority()\n",
    "        is_weight = np.power(self.buffer_size * sampling_probabilities, -beta)\n",
    "        is_weight /= is_weight.max()  # 정규화\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def compute_loss(self, batch, idxs, is_weight, beta):\n",
    "        # 배치에서 데이터 추출\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        # Q 값 계산\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(dim=1)[0]\n",
    "            targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        # TD 오차 계산 및 우선순위 업데이트\n",
    "        td_errors = torch.abs(targets - q_values)\n",
    "        loss = (torch.FloatTensor(is_weight).to(device) * (q_values - targets.detach()) ** 2).mean()\n",
    "        \n",
    "        for idx, td_error in zip(idxs, td_errors.detach().cpu().numpy()):\n",
    "            priority = td_error.item() ** self.alpha\n",
    "            self.memory.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        if self.memory.data_pointer < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Beta 값을 학습 진행에 따라 증가\n",
    "        beta = self.beta_start + (self.beta_end - self.beta_start) * min(1.0, self.step_count / 50000)\n",
    "        batch, idxs, is_weight = self.sample_batch(beta)\n",
    "        loss = self.compute_loss(batch, idxs, is_weight, beta)\n",
    "        \n",
    "        # 모델 업데이트\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# 학습 루프 정의 (변경 없음)\n",
    "def train_dqn(env, agent, max_episodes=20000):\n",
    "    rewards = []\n",
    "    path_lengths = []\n",
    "    epsilon = agent.epsilon_start\n",
    "    \n",
    "    image_dir = r\"C:\\baramproject\\trained_model\\sibal21_2\\episode_debug_image\"\n",
    "    data_dir = r\"C:\\baramproject\\trained_model\\sibal21_2\\episode_debug_data\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for episode in tqdm(range(max_episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path_length = 0\n",
    "        done = False\n",
    "        debug_data = []\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = max(agent.epsilon_end, epsilon - (agent.epsilon_start - agent.epsilon_end) / agent.epsilon_decay)\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            q_values = agent.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\n",
    "            debug_data.append({\n",
    "                \"step\": path_length,\n",
    "                \"state\": state.tolist(),\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"next_state\": next_state.tolist(),\n",
    "                \"q_values\": q_values.tolist(),\n",
    "                \"epsilon\": epsilon\n",
    "            })\n",
    "            \n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path_length += 1\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        path_lengths.append(path_length)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Path Length: {path_length}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(env.grid, cmap='gray')\n",
    "            path_array = np.array(env.path)\n",
    "            plt.plot(path_array[:, 1], path_array[:, 0], 'r-', label='Path')\n",
    "            plt.plot(env.start_pos[1], env.start_pos[0], 'go', label='Start')\n",
    "            plt.plot(env.end_pos[1], env.end_pos[0], 'bo', label='End')\n",
    "            plt.legend()\n",
    "            plt.title(f\"Episode {episode} Path\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            with open(os.path.join(data_dir, f\"episode_{episode}.json\"), 'w') as f:\n",
    "                json.dump(debug_data, f, indent=4)\n",
    "        \n",
    "        if episode % 1000 == 0 and episode > 0:\n",
    "            plt.plot(rewards)\n",
    "            plt.title(\"Total Rewards Over Episodes\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"rewards_episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), r\"C:\\baramproject\\trained_model\\sibal21_2\\navigation_model.pth\")\n",
    "    return rewards, path_lengths\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    env = NavigationEnv()\n",
    "    state_dim = 7\n",
    "    action_dim = len(env.action_space)\n",
    "    model_path = r\"C:\\baramproject\\trained_model\\sibal21\\navigation_model.pth\"\n",
    "    agent = DQNAgent(state_dim, action_dim, model_path=model_path)\n",
    "    rewards, path_lengths = train_dqn(env, agent)\n",
    "    \n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Total Rewards Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(path_lengths) \n",
    "    plt.title(\"Path Lengths Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Path Length\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
