{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d8ee47-4340-4bd2-b2fe-b402c620e0b9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"18189d24-d525-4bd1-a837-6af29603aba8\",\n",
    "   \"metadata\": {\n",
    "    \"scrolled\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from math import atan2, degrees, radians, cos, sin\\n\",\n",
    "    \"from datetime import datetime, timedelta\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.optim as optim\\n\",\n",
    "    \"import random\\n\",\n",
    "    \"from collections import deque, namedtuple\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# CUDA 디바이스 설정\\n\",\n",
    "    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 경험 저장을 위한 named tuple 정의\\n\",\n",
    "    \"Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Dueling DQN 네트워크 정의\\n\",\n",
    "    \"class DuelingDQN(nn.Module):\\n\",\n",
    "    \"    def __init__(self, state_dim, action_dim):\\n\",\n",
    "    \"        super(DuelingDQN, self).__init__()\\n\",\n",
    "    \"        self.fc1 = nn.Linear(state_dim, 128)\\n\",\n",
    "    \"        self.fc2 = nn.Linear(128, 64)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 상태 가치 스트림\\n\",\n",
    "    \"        self.value_stream = nn.Linear(64, 1)\\n\",\n",
    "    \"        # 액션 이점 스트림\\n\",\n",
    "    \"        self.advantage_stream = nn.Linear(64, action_dim)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        x = torch.relu(self.fc1(x))\\n\",\n",
    "    \"        x = torch.relu(self.fc2(x))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        value = self.value_stream(x)\\n\",\n",
    "    \"        advantage = self.advantage_stream(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Q 값 계산: V(s) + (A(s,a) - mean(A(s)))\\n\",\n",
    "    \"        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\\n\",\n",
    "    \"        return q_values\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 항해 환경 클래스 정의\\n\",\n",
    "    \"class NavigationEnv:\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        # 그리드 맵 로드\\n\",\n",
    "    \"        self.grid = np.load('land_sea_grid_cartopy_downsized.npy')\\n\",\n",
    "    \"        self.n_rows, self.n_cols = self.grid.shape\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 경도/위도 범위\\n\",\n",
    "    \"        self.lat_min, self.lat_max = 30, 38\\n\",\n",
    "    \"        self.lon_min, self.lon_max = 120, 127\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 시작점과 종료점 설정\\n\",\n",
    "    \"        self.start_pos = self.latlon_to_grid(37.46036, 126.52360)\\n\",\n",
    "    \"        self.end_pos = self.latlon_to_grid(30.62828, 122.06400)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 시간 관리\\n\",\n",
    "    \"        self.step_time_minutes = 12\\n\",\n",
    "    \"        self.max_steps = 300\\n\",\n",
    "    \"        self.cumulative_time = 0\\n\",\n",
    "    \"        self.step_count = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 조류 및 풍향/풍속 데이터 경로\\n\",\n",
    "    \"        self.tidal_data_dir = r\\\"C:\\\\baramproject\\\\tidal_database\\\"\\n\",\n",
    "    \"        self.wind_data_dir = r\\\"C:\\\\baramproject\\\\wind_database_2\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 액션 공간 정의 (상대 각도: 종료점 방향 기준 8방향)\\n\",\n",
    "    \"        self.action_space = np.array([0, 45, 90, 135, 180, -135, -90, -45])  # 상대 각도 (도 단위)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 그리드 이동 방향 매핑 (상, 우상, 우, 우하, 하, 좌하, 좌, 좌상)\\n\",\n",
    "    \"        self.grid_directions = [\\n\",\n",
    "    \"            (-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 연비 효율 관련 계수\\n\",\n",
    "    \"        self.k_c = 0.1  # 조류 영향 계수\\n\",\n",
    "    \"        self.k_w = 0.005  # 풍속 영향 계수\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 경로 저장용 리스트\\n\",\n",
    "    \"        self.path = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 환경 초기화\\n\",\n",
    "    \"        self.reset()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def latlon_to_grid(self, lat, lon):\\n\",\n",
    "    \"        \\\"\\\"\\\"위도/경도를 그리드 좌표로 변환\\\"\\\"\\\"\\n\",\n",
    "    \"        row = int((self.lat_max - lat) / (self.lat_max - self.lat_min) * self.n_rows)\\n\",\n",
    "    \"        col = int((lon - self.lon_min) / (self.lon_max - self.lon_min) * self.n_cols)\\n\",\n",
    "    \"        return row, col\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def reset(self, start_time=None):\\n\",\n",
    "    \"        start_date = datetime(2018, 1, 1, 0, 0)\\n\",\n",
    "    \"        end_date = datetime(2018, 12, 29, 0, 0)  # 12월 29일로 변경하여 여유를 둠\\n\",\n",
    "    \"        if start_time is None:\\n\",\n",
    "    \"            time_delta = (end_date - start_date).total_seconds()\\n\",\n",
    "    \"            random_seconds = np.random.randint(0, int(time_delta / 60 / 30) + 1) * 30 * 60\\n\",\n",
    "    \"            start_time = start_date + timedelta(seconds=random_seconds)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.current_pos = self.start_pos\\n\",\n",
    "    \"        self.visit_count = {}\\n\",\n",
    "    \"        self.prev_action = None\\n\",\n",
    "    \"        self.current_time = start_time\\n\",\n",
    "    \"        self.cumulative_time = 0\\n\",\n",
    "    \"        self.load_tidal_data()\\n\",\n",
    "    \"        self.map_tidal_to_grid()\\n\",\n",
    "    \"        self.load_wind_data()\\n\",\n",
    "    \"        self.map_wind_to_grid()\\n\",\n",
    "    \"        self.prev_distance = self.get_distance_to_end()\\n\",\n",
    "    \"        self.step_count = 0\\n\",\n",
    "    \"        self.path = [self.current_pos]\\n\",\n",
    "    \"        return self._get_state()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def get_relative_position_and_angle(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"종료점을 기준으로 한 상대 좌표와 각도 계산\\\"\\\"\\\"\\n\",\n",
    "    \"        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\\n\",\n",
    "    \"        distance = np.linalg.norm(rel_pos)\\n\",\n",
    "    \"        end_angle = degrees(atan2(rel_pos[1], rel_pos[0])) % 360\\n\",\n",
    "    \"        return rel_pos, distance, end_angle\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def get_distance_to_end(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"종료점까지의 거리 계산\\\"\\\"\\\"\\n\",\n",
    "    \"        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\\n\",\n",
    "    \"        return np.linalg.norm(rel_pos)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def angle_to_grid_direction(self, abs_action_angle):\\n\",\n",
    "    \"        \\\"\\\"\\\"절대 각도를 그리드 이동 방향으로 매핑\\\"\\\"\\\"\\n\",\n",
    "    \"        grid_angles = np.array([0, 45, 90, 135, 180, 225, 270, 315])\\n\",\n",
    "    \"        angle_diff = np.abs(grid_angles - abs_action_angle)\\n\",\n",
    "    \"        closest_idx = np.argmin(angle_diff)\\n\",\n",
    "    \"        return self.grid_directions[closest_idx]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def load_data(self, data_dir, filename_prefix, time_str):\\n\",\n",
    "    \"        data_file = os.path.join(data_dir, f\\\"{filename_prefix}{time_str}.json\\\")\\n\",\n",
    "    \"        if not os.path.exists(data_file):\\n\",\n",
    "    \"            print(f\\\"Warning: Data file {data_file} not found. Episode will be terminated.\\\")\\n\",\n",
    "    \"            return None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        with open(data_file, 'r') as f:\\n\",\n",
    "    \"            data = json.load(f)\\n\",\n",
    "    \"        return data[\\\"result\\\"][\\\"data\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def map_data_to_grid(self, data, dir_key, speed_key):\\n\",\n",
    "    \"        \\\"\\\"\\\"공통 데이터 매핑 함수\\\"\\\"\\\"\\n\",\n",
    "    \"        grid_dir = np.zeros((self.n_rows, self.n_cols))\\n\",\n",
    "    \"        grid_speed = np.zeros((self.n_rows, self.n_cols))\\n\",\n",
    "    \"        grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if data is None:\\n\",\n",
    "    \"            return grid_dir, grid_speed, grid_valid\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        positions = [(float(item[\\\"pre_lat\\\"]), float(item[\\\"pre_lon\\\"])) for item in data]\\n\",\n",
    "    \"        directions = [float(item[dir_key]) for item in data]\\n\",\n",
    "    \"        speeds = [float(item[speed_key]) for item in data]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for pos, dir, speed in zip(positions, directions, speeds):\\n\",\n",
    "    \"            lat, lon = pos\\n\",\n",
    "    \"            row, col = self.latlon_to_grid(lat, lon)\\n\",\n",
    "    \"            if 0 <= row < self.n_rows and 0 <= col < self.n_cols:\\n\",\n",
    "    \"                grid_dir[row, col] = dir\\n\",\n",
    "    \"                grid_speed[row, col] = speed\\n\",\n",
    "    \"                grid_valid[row, col] = True\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return grid_dir, grid_speed, grid_valid\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def load_tidal_data(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"조류 데이터 로드\\\"\\\"\\\"\\n\",\n",
    "    \"        time_str = self.current_time.strftime(\\\"%Y%m%d_%H%M\\\")\\n\",\n",
    "    \"        tidal_data = self.load_data(self.tidal_data_dir, \\\"tidal_\\\", time_str)\\n\",\n",
    "    \"        if tidal_data is not None:\\n\",\n",
    "    \"            self.tidal_data = tidal_data\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            self.tidal_data = None  # 데이터가 없으면 None으로 설정\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def map_tidal_to_grid(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"조류 데이터를 그리드에 매핑\\\"\\\"\\\"\\n\",\n",
    "    \"        if self.tidal_data is not None:\\n\",\n",
    "    \"            self.tidal_grid_dir, self.tidal_grid_speed, self.tidal_grid_valid = self.map_data_to_grid(\\n\",\n",
    "    \"                self.tidal_data, \\\"current_dir\\\", \\\"current_speed\\\"\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            self.tidal_grid_dir = np.zeros((self.n_rows, self.n_cols))\\n\",\n",
    "    \"            self.tidal_grid_speed = np.zeros((self.n_rows, self.n_cols))\\n\",\n",
    "    \"            self.tidal_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def load_wind_data(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"풍향/풍속 데이터 로드\\\"\\\"\\\"\\n\",\n",
    "    \"        time_str = self.current_time.strftime(\\\"%Y%m%d_%H%M\\\")\\n\",\n",
    "    \"        wind_data = self.load_data(self.wind_data_dir, \\\"wind_\\\", time_str)\\n\",\n",
    "    \"        if wind_data is not None:\\n\",\n",
    "    \"            self.wind_data = wind_data\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            self.wind_data = None  # 데이터가 없으면 None으로 설정\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def map_wind_to_grid(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"풍향/풍속 데이터를 그리드에 매핑\\\"\\\"\\\"\\n\",\n",
    "    \"        if self.wind_data is not None:\\n\",\n",
    "    \"            self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid = self.map_data_to_grid(\\n\",\n",
    "    \"                self.wind_data, \\\"wind_dir\\\", \\\"wind_speed\\\"\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            self.wind_grid_dir = np.zeros((self.n_rows, self.n_cols))\\n\",\n",
    "    \"            self.wind_grid_speed = np.zeros((self.n_rows, self.n_cols))\\n\",\n",
    "    \"            self.wind_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def calculate_fuel_consumption(self, abs_action_angle, position):\\n\",\n",
    "    \"        \\\"\\\"\\\"연료 소비 계산\\\"\\\"\\\"\\n\",\n",
    "    \"        row, col = position\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        tidal_dir, tidal_speed = 0, 0\\n\",\n",
    "    \"        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.tidal_grid_valid[row, col]:\\n\",\n",
    "    \"            tidal_dir = self.tidal_grid_dir[row, col]\\n\",\n",
    "    \"            tidal_speed = self.tidal_grid_speed[row, col]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        wind_dir, wind_speed = 0, 0\\n\",\n",
    "    \"        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.wind_grid_valid[row, col]:\\n\",\n",
    "    \"            wind_dir = self.wind_grid_dir[row, col]\\n\",\n",
    "    \"            wind_speed = self.wind_grid_speed[row, col]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        tidal_dir_rad = (90 - tidal_dir) * np.pi / 180\\n\",\n",
    "    \"        wind_dir_rad = (90 - wind_dir) * np.pi / 180\\n\",\n",
    "    \"        action_angle_rad = (90 - abs_action_angle) * np.pi / 180\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        theta_c = action_angle_rad - tidal_dir_rad\\n\",\n",
    "    \"        theta_w = action_angle_rad - wind_dir_rad\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        f_0 = 1\\n\",\n",
    "    \"        tidal_effect = -self.k_c * tidal_speed * cos(theta_c)\\n\",\n",
    "    \"        wind_effect = self.k_w * wind_speed * cos(theta_w)\\n\",\n",
    "    \"        total_fuel = f_0 + wind_effect + tidal_effect\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return total_fuel\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def step(self, action):\\n\",\n",
    "    \"        \\\"\\\"\\\"환경 스텝 실행\\\"\\\"\\\"\\n\",\n",
    "    \"        # 스텝 수 증가\\n\",\n",
    "    \"        self.step_count += 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"        # 상대 위치 및 각도 계산\\n\",\n",
    "    \"        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\\n\",\n",
    "    \"        rel_action_angle = self.action_space[action]\\n\",\n",
    "    \"        abs_action_angle = (end_angle + rel_action_angle) % 360\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 턴 페널티 계산 (이전 방향이 있을 경우)\\n\",\n",
    "    \"        turn_penalty = 0\\n\",\n",
    "    \"        if hasattr(self, 'previous_direction') and self.previous_direction is not None:\\n\",\n",
    "    \"            angle_diff = min((abs_action_angle - self.previous_direction) % 360, \\n\",\n",
    "    \"                             (self.previous_direction - abs_action_angle) % 360)\\n\",\n",
    "    \"            turn_penalty = angle_diff * 0.1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 그리드 이동 방향 계산\\n\",\n",
    "    \"        move_dir = self.angle_to_grid_direction(abs_action_angle)\\n\",\n",
    "    \"        new_pos = (self.current_pos[0] + move_dir[0], self.current_pos[1] + move_dir[1])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 연료 소비 계산\\n\",\n",
    "    \"        current_fuel = self.calculate_fuel_consumption(abs_action_angle, self.current_pos)\\n\",\n",
    "    \"        next_fuel = self.calculate_fuel_consumption(abs_action_angle, new_pos)\\n\",\n",
    "    \"        fuel_reduction = current_fuel - next_fuel\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 새 위치가 유효한지 확인하고 이동\\n\",\n",
    "    \"        if (0 <= new_pos[0] < self.n_rows and 0 <= new_pos[1] < self.n_cols and \\n\",\n",
    "    \"            self.grid[new_pos[0], new_pos[1]] == 0):\\n\",\n",
    "    \"            self.current_pos = new_pos\\n\",\n",
    "    \"            self.path.append(self.current_pos)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 이전 방향 업데이트\\n\",\n",
    "    \"        self.previous_direction = abs_action_angle\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 이전 액션 업데이트\\n\",\n",
    "    \"        self.prev_action = action\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 시간 업데이트\\n\",\n",
    "    \"        self.cumulative_time += self.step_time_minutes\\n\",\n",
    "    \"        if self.cumulative_time >= 30:\\n\",\n",
    "    \"            next_time = self.current_time + timedelta(minutes=30)\\n\",\n",
    "    \"            end_date = datetime(2018, 12, 31, 23, 30)\\n\",\n",
    "    \"            if next_time <= end_date:\\n\",\n",
    "    \"                self.current_time = next_time\\n\",\n",
    "    \"                self.load_tidal_data()\\n\",\n",
    "    \"                self.map_tidal_to_grid()\\n\",\n",
    "    \"                self.load_wind_data()\\n\",\n",
    "    \"                self.map_wind_to_grid()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                print(\\\"Warning: Time exceeds 2018 range. Keeping previous data.\\\")\\n\",\n",
    "    \"            self.cumulative_time -= 30\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 상태, 보상, 종료 여부 계산\\n\",\n",
    "    \"        state = self._get_state()\\n\",\n",
    "    \"        current_distance = self.get_distance_to_end()\\n\",\n",
    "    \"        distance_reward = (self.prev_distance - current_distance) * 2.0\\n\",\n",
    "    \"        self.prev_distance = current_distance\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        goal_reward = 100 if tuple(self.current_pos) == self.end_pos else 0\\n\",\n",
    "    \"        reward = -current_fuel + fuel_reduction * 1.0 + distance_reward - turn_penalty + goal_reward\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 종료 조건: 목표 도달 또는 스텝 수 300 초과\\n\",\n",
    "    \"        done = tuple(self.current_pos) == self.end_pos or self.step_count >= self.max_steps\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return state, reward, done, {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def _get_state(self):\\n\",\n",
    "    \"        row, col = self.current_pos\\n\",\n",
    "    \"        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        tidal_dir, tidal_speed = 0, 0\\n\",\n",
    "    \"        if hasattr(self, 'tidal_grid_valid') and self.tidal_grid_valid[row, col]:\\n\",\n",
    "    \"            tidal_dir = self.tidal_grid_dir[row, col]\\n\",\n",
    "    \"            tidal_speed = self.tidal_grid_speed[row, col]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        wind_dir, wind_speed = 0, 0\\n\",\n",
    "    \"        if hasattr(self, 'wind_grid_valid') and self.wind_grid_valid[row, col]:\\n\",\n",
    "    \"            wind_dir = self.wind_grid_dir[row, col]\\n\",\n",
    "    \"            wind_speed = self.wind_grid_speed[row, col]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return np.array([rel_pos[0], rel_pos[1], distance, tidal_dir, tidal_speed, wind_dir, wind_speed])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# DQN 에이전트 클래스 정의\\n\",\n",
    "    \"class DQNAgent:\\n\",\n",
    "    \"    def __init__(self, state_dim, action_dim):\\n\",\n",
    "    \"        self.state_dim = state_dim\\n\",\n",
    "    \"        self.action_dim = action_dim\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.lr = 0.0001\\n\",\n",
    "    \"        self.gamma = 0.99\\n\",\n",
    "    \"        self.batch_size = 64\\n\",\n",
    "    \"        self.buffer_size = 100000\\n\",\n",
    "    \"        self.target_update = 1000\\n\",\n",
    "    \"        self.epsilon_start = 1.0\\n\",\n",
    "    \"        self.epsilon_end = 0.01\\n\",\n",
    "    \"        self.epsilon_decay = 10000\\n\",\n",
    "    \"        self.n_steps = 3\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.alpha = 0.6\\n\",\n",
    "    \"        self.beta_start = 0.4\\n\",\n",
    "    \"        self.beta_end = 1.0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 모델을 GPU로 이동\\n\",\n",
    "    \"        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\\n\",\n",
    "    \"        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\\n\",\n",
    "    \"        self.target_net.load_state_dict(self.policy_net.state_dict())\\n\",\n",
    "    \"        self.target_net.eval()\\n\",\n",
    "    \"        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.memory = deque(maxlen=self.buffer_size)\\n\",\n",
    "    \"        self.step_count = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def select_action(self, state, epsilon):\\n\",\n",
    "    \"        \\\"\\\"\\\"액션 선택\\\"\\\"\\\"\\n\",\n",
    "    \"        self.step_count += 1\\n\",\n",
    "    \"        if random.random() < epsilon:\\n\",\n",
    "    \"            return random.randrange(self.action_dim)\\n\",\n",
    "    \"        state = torch.FloatTensor(state).unsqueeze(0).to(device)\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            q_values = self.policy_net(state)\\n\",\n",
    "    \"        return q_values.argmax().item()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def store_experience(self, state, action, reward, next_state, done):\\n\",\n",
    "    \"        \\\"\\\"\\\"경험 저장\\\"\\\"\\\"\\n\",\n",
    "    \"        experience = Experience(state, action, reward, next_state, done)\\n\",\n",
    "    \"        self.memory.append((experience, 1.0))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def sample_batch(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"배치 샘플링\\\"\\\"\\\"\\n\",\n",
    "    \"        batch = random.sample(self.memory, min(len(self.memory), self.batch_size))\\n\",\n",
    "    \"        experiences, priorities = zip(*batch)\\n\",\n",
    "    \"        return experiences\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def compute_loss(self, batch, beta):\\n\",\n",
    "    \"        \\\"\\\"\\\"손실 계산\\\"\\\"\\\"\\n\",\n",
    "    \"        states, actions, rewards, next_states, dones = zip(*batch)\\n\",\n",
    "    \"        states = torch.FloatTensor(states).to(device)\\n\",\n",
    "    \"        actions = torch.LongTensor(actions).to(device)\\n\",\n",
    "    \"        rewards = torch.FloatTensor(rewards).to(device)\\n\",\n",
    "    \"        next_states = torch.FloatTensor(next_states).to(device)\\n\",\n",
    "    \"        dones = torch.FloatTensor(dones).to(device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\\n\",\n",
    "    \"        next_q_values = self.policy_net(next_states).max(1)[1].unsqueeze(1)\\n\",\n",
    "    \"        target_next_q_values = self.target_net(next_states).gather(1, next_q_values)\\n\",\n",
    "    \"        targets = rewards + (1 - dones) * self.gamma * target_next_q_values.squeeze()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        loss = nn.MSELoss()(q_values.squeeze(), targets.detach())\\n\",\n",
    "    \"        return loss\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def update(self):\\n\",\n",
    "    \"        \\\"\\\"\\\"모델 업데이트\\\"\\\"\\\"\\n\",\n",
    "    \"        if len(self.memory) < self.batch_size:\\n\",\n",
    "    \"            return\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        beta = self.beta_start + (self.beta_end - self.beta_start) * min(1.0, self.step_count / 50000)\\n\",\n",
    "    \"        batch = self.sample_batch()\\n\",\n",
    "    \"        loss = self.compute_loss(batch, beta)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.optimizer.zero_grad()\\n\",\n",
    "    \"        loss.backward()\\n\",\n",
    "    \"        self.optimizer.step()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if self.step_count % self.target_update == 0:\\n\",\n",
    "    \"            self.target_net.load_state_dict(self.policy_net.state_dict())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 학습 루프 정의\\n\",\n",
    "    \"def train_dqn(env, agent, max_episodes=20000):\\n\",\n",
    "    \"    rewards = []\\n\",\n",
    "    \"    path_lengths = []\\n\",\n",
    "    \"    epsilon = agent.epsilon_start\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    image_dir = r\\\"C:\\\\baramproject\\\\trained_model\\\\sibal17\\\\episode_debug_image\\\"\\n\",\n",
    "    \"    data_dir = r\\\"C:\\\\baramproject\\\\trained_model\\\\sibal17\\\\episode_debug_data\\\"\\n\",\n",
    "    \"    os.makedirs(image_dir, exist_ok=True)\\n\",\n",
    "    \"    os.makedirs(data_dir, exist_ok=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for episode in tqdm(range(max_episodes), desc=\\\"Training Episodes\\\"):\\n\",\n",
    "    \"        state = env.reset()\\n\",\n",
    "    \"        total_reward = 0\\n\",\n",
    "    \"        path_length = 0\\n\",\n",
    "    \"        done = False\\n\",\n",
    "    \"        debug_data = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        while not done:\\n\",\n",
    "    \"            epsilon = max(agent.epsilon_end, epsilon - (agent.epsilon_start - agent.epsilon_end) / agent.epsilon_decay)\\n\",\n",
    "    \"            action = agent.select_action(state, epsilon)\\n\",\n",
    "    \"            next_state, reward, done, _ = env.step(action)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            q_values = agent.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\\n\",\n",
    "    \"            debug_data.append({\\n\",\n",
    "    \"                \\\"step\\\": path_length,\\n\",\n",
    "    \"                \\\"state\\\": state.tolist(),\\n\",\n",
    "    \"                \\\"action\\\": action,\\n\",\n",
    "    \"                \\\"reward\\\": reward,\\n\",\n",
    "    \"                \\\"next_state\\\": next_state.tolist(),\\n\",\n",
    "    \"                \\\"q_values\\\": q_values.tolist(),\\n\",\n",
    "    \"                \\\"epsilon\\\": epsilon\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            agent.store_experience(state, action, reward, next_state, done)\\n\",\n",
    "    \"            agent.update()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            state = next_state\\n\",\n",
    "    \"            total_reward += reward\\n\",\n",
    "    \"            path_length += 1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        rewards.append(total_reward)\\n\",\n",
    "    \"        path_lengths.append(path_length)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if episode % 100 == 0:\\n\",\n",
    "    \"            print(f\\\"Episode {episode}, Total Reward: {total_reward}, Path Length: {path_length}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"            plt.imshow(env.grid, cmap='gray')\\n\",\n",
    "    \"            path_array = np.array(env.path)\\n\",\n",
    "    \"            plt.plot(path_array[:, 1], path_array[:, 0], 'r-', label='Path')\\n\",\n",
    "    \"            plt.plot(env.start_pos[1], env.start_pos[0], 'go', label='Start')\\n\",\n",
    "    \"            plt.plot(env.end_pos[1], env.end_pos[0], 'bo', label='End')\\n\",\n",
    "    \"            plt.legend()\\n\",\n",
    "    \"            plt.title(f\\\"Episode {episode} Path\\\")\\n\",\n",
    "    \"            plt.savefig(os.path.join(image_dir, f\\\"episode_{episode}.png\\\"))\\n\",\n",
    "    \"            plt.close()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            with open(os.path.join(data_dir, f\\\"episode_{episode}.json\\\"), 'w') as f:\\n\",\n",
    "    \"                json.dump(debug_data, f, indent=4)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if episode % 1000 == 0 and episode > 0:\\n\",\n",
    "    \"            plt.plot(rewards)\\n\",\n",
    "    \"            plt.title(\\\"Total Rewards Over Episodes\\\")\\n\",\n",
    "    \"            plt.xlabel(\\\"Episode\\\")\\n\",\n",
    "    \"            plt.ylabel(\\\"Reward\\\")\\n\",\n",
    "    \"            plt.savefig(os.path.join(image_dir, f\\\"rewards_episode_{episode}.png\\\"))\\n\",\n",
    "    \"            plt.close()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    torch.save(agent.policy_net.state_dict(), r\\\"C:\\\\baramproject\\\\trained_model\\\\sibal17\\\\navigation_model.pth\\\")\\n\",\n",
    "    \"    return rewards, path_lengths\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 메인 실행\\n\",\n",
    "    \"if __name__ == \\\"__main__\\\":\\n\",\n",
    "    \"    env = NavigationEnv()\\n\",\n",
    "    \"    state_dim = 7\\n\",\n",
    "    \"    action_dim = len(env.action_space)\\n\",\n",
    "    \"    agent = DQNAgent(state_dim, action_dim)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    rewards, path_lengths = train_dqn(env, agent)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.plot(rewards)\\n\",\n",
    "    \"    plt.title(\\\"Total Rewards Over Episodes\\\")\\n\",\n",
    "    \"    plt.xlabel(\\\"Episode\\\")\\n\",\n",
    "    \"    plt.ylabel(\\\"Reward\\\")\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.plot(path_lengths)\\n\",\n",
    "    \"    plt.title(\\\"Path Lengths Over Episodes\\\")\\n\",\n",
    "    \"    plt.xlabel(\\\"Episode\\\")\\n\",\n",
    "    \"    plt.ylabel(\\\"Path Length\\\")\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.13\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
