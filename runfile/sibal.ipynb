{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fb9b1e-f289-4c08-b60e-c2b032e57306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▍                                | 75/5000 [03:04<3:22:12,  2.46s/it, Reward=592.18, Success=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 428\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 428\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOAD_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 377\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, save_interval, tidal_base_path, load_model_path, checkpoint_path)\u001b[0m\n\u001b[0;32m    375\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (weights \u001b[38;5;241m*\u001b[39m (q_values \u001b[38;5;241m-\u001b[39m targets) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    376\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 377\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    379\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mupdate_priorities(indices, td_errors)\n",
      "File \u001b[1;32mC:\\baramproject\\sibal\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mC:\\baramproject\\sibal\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mC:\\baramproject\\sibal\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "GRID_SIZE = 100\n",
    "NUM_OBSTACLES = int(GRID_SIZE * GRID_SIZE * 0.00555)\n",
    "LAT_RANGE = (33.0, 38.0)\n",
    "LON_RANGE = (120.0, 127.0)\n",
    "TRAINING_EPISODES = 5000\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.995\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.9995\n",
    "LEARNING_RATE = 0.0001\n",
    "SAVE_INTERVAL = 1000\n",
    "MAX_STEPS = GRID_SIZE * 2\n",
    "\n",
    "# 파일 경로\n",
    "TIDAL_BASE_PATH = \"C:/baramproject/tidal_database\"\n",
    "CHECKPOINT_PATH = \"C:/baramproject/trained_model/sibal/checkpoint.pth\"\n",
    "LOAD_MODEL_PATH = None\n",
    "\n",
    "# 시작점과 목표점 좌표\n",
    "START_GOAL_COORDS = [\n",
    "    ((37.460359, 126.623605), (33.56036, 120.57860)),\n",
    "    ((33.56036, 120.57860), (37.460359, 126.623605))\n",
    "]\n",
    "\n",
    "# CUDA 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using Cuda\")\n",
    "else:\n",
    "    print(\"Cuda not available, using CPU\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# 격자 맵 생성 및 유틸리티 함수 (변경 없음)\n",
    "def create_grid_map(size=GRID_SIZE, num_obstacles=NUM_OBSTACLES):\n",
    "    grid = np.zeros((size, size), dtype=np.float32)\n",
    "    for _ in range(num_obstacles):\n",
    "        x, y = random.randint(0, size-1), random.randint(0, size-1)\n",
    "        grid[x, y] = 1\n",
    "    return grid\n",
    "\n",
    "def latlon_to_grid(lat, lon, lat_range=LAT_RANGE, lon_range=LON_RANGE, grid_size=GRID_SIZE):\n",
    "    lat_min, lat_max = lat_range\n",
    "    lon_min, lon_max = lon_range\n",
    "    x = int((lon - lon_min) / (lon_max - lon_min) * (grid_size - 1))\n",
    "    y = int((lat_max - lat) / (lat_max - lat_min) * (grid_size - 1))\n",
    "    return x, y\n",
    "\n",
    "directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
    "action_to_direction = {i: d for i, d in enumerate(directions)}\n",
    "\n",
    "def move(position, action):\n",
    "    x, y = position\n",
    "    moves = {0: (0, -1), 1: (1, -1), 2: (1, 0), 3: (1, 1),\n",
    "             4: (0, 1), 5: (-1, 1), 6: (-1, 0), 7: (-1, -1)}\n",
    "    dx, dy = moves[action]\n",
    "    return x + dx, y + dy\n",
    "\n",
    "def load_tidal_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['result']['data']\n",
    "\n",
    "def get_random_tidal_file(base_path=TIDAL_BASE_PATH):\n",
    "    start_date = datetime(2018, 1, 1, 0, 0)\n",
    "    end_date = datetime(2018, 12, 31, 23, 30)\n",
    "    total_half_hours = int((end_date - start_date).total_seconds() / 1800) + 1  # 1800초 = 30분\n",
    "    random_idx = random.randint(0, total_half_hours - 1)\n",
    "    random_date = start_date + timedelta(minutes=random_idx * 30)  # 30분 단위로 변경\n",
    "    file_name = f\"tidal_{random_date.strftime('%Y%m%d_%H%M')}.json\"\n",
    "    return os.path.join(base_path, file_name)\n",
    "\n",
    "def get_tidal_effect(position, tidal_data, lat_range=LAT_RANGE, lon_range=LON_RANGE, grid_size=GRID_SIZE):\n",
    "    x, y = position\n",
    "    lat = lat_range[1] - (y / (grid_size - 1)) * (lat_range[1] - lat_range[0])\n",
    "    lon = lon_range[0] + (x / (grid_size - 1)) * (lon_range[1] - lon_range[0])\n",
    "    min_dist = float('inf')\n",
    "    tidal_dir, tidal_speed = 0, 0\n",
    "    for entry in tidal_data:\n",
    "        entry_lat = float(entry['pre_lat'])\n",
    "        entry_lon = float(entry['pre_lon'])\n",
    "        dist = (lat - entry_lat) ** 2 + (lon - entry_lon) ** 2\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            tidal_dir = float(entry['current_dir'])\n",
    "            tidal_speed = float(entry['current_speed'])\n",
    "    return tidal_dir / 360.0, tidal_speed / 100.0\n",
    "\n",
    "def get_adjacent_tidal_info(position, tidal_data, grid_size=GRID_SIZE):\n",
    "    x, y = position\n",
    "    directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "    tidal_dirs = []\n",
    "    tidal_speeds = []\n",
    "    for dx, dy in directions:\n",
    "        adj_pos = (x + dx, y + dy)\n",
    "        if 0 <= adj_pos[0] < grid_size and 0 <= adj_pos[1] < grid_size:\n",
    "            dir_, speed_ = get_tidal_effect(adj_pos, tidal_data)\n",
    "            tidal_dirs.append(dir_)\n",
    "            tidal_speeds.append(speed_)\n",
    "        else:\n",
    "            tidal_dirs.append(0.0)\n",
    "            tidal_speeds.append(0.0)\n",
    "    return tidal_dirs, tidal_speeds\n",
    "\n",
    "def get_obstacle_info(position, grid, window_size=5):\n",
    "    x, y = position\n",
    "    obstacles = []\n",
    "    for dx in range(-2, 3):\n",
    "        for dy in range(-2, 3):\n",
    "            if dx == 0 and dy == 0:\n",
    "                continue\n",
    "            adj_pos = (x + dx, y + dy)\n",
    "            if 0 <= adj_pos[0] < GRID_SIZE and 0 <= adj_pos[1] < GRID_SIZE:\n",
    "                obstacles.append(int(grid[adj_pos]))\n",
    "            else:\n",
    "                obstacles.append(1)\n",
    "    return obstacles\n",
    "\n",
    "def calculate_bearing_diff(current_dir, goal_pos, current_pos):\n",
    "    dx_goal = goal_pos[0] - current_pos[0]\n",
    "    dy_goal = goal_pos[1] - current_pos[1]\n",
    "    bearing_to_goal = np.arctan2(dy_goal, dx_goal) * 180 / np.pi\n",
    "    if bearing_to_goal < 0:\n",
    "        bearing_to_goal += 360\n",
    "    angle_diff = abs(current_dir * 45 - bearing_to_goal)\n",
    "    angle_diff = min(angle_diff, 360 - angle_diff)\n",
    "    return angle_diff / 180.0\n",
    "\n",
    "# 보상 함수\n",
    "def calculate_reward(grid, current_pos, action, next_pos, prev_action, tidal_data, goal, step_count, max_steps=MAX_STEPS):\n",
    "    size = grid.shape[0]\n",
    "    #충돌, 맵이탈\n",
    "    if (next_pos[0] < 0 or next_pos[0] >= size or \n",
    "        next_pos[1] < 0 or next_pos[1] >= size or \n",
    "        grid[next_pos] == 1):\n",
    "        return -50\n",
    "    #목표도달\n",
    "    if next_pos == goal:\n",
    "        return 100\n",
    "    #근방 장애물 패널티\n",
    "    obstacle_penalty = 0\n",
    "    for dx in [-1, 0, 1]:\n",
    "        for dy in [-1, 0, 1]:\n",
    "            if dx == 0 and dy == 0:\n",
    "                continue\n",
    "            adj_pos = (next_pos[0] + dx, next_pos[1] + dy)\n",
    "            if 0 <= adj_pos[0] < size and 0 <= adj_pos[1] < size and grid[adj_pos] == 1:\n",
    "                obstacle_penalty = -10\n",
    "                break\n",
    "    #방향전환 패널티\n",
    "    direction_penalty = 0\n",
    "    if prev_action is not None:\n",
    "        angle_diff = abs(action - prev_action) * 45\n",
    "        angle_diff = min(angle_diff, 360 - angle_diff)\n",
    "        if angle_diff > 45:\n",
    "            direction_penalty = -2\n",
    "    #종료점까지 방향 보상\n",
    "    bearing_diff = calculate_bearing_diff(action, goal, next_pos)\n",
    "    bearing_reward = (1 - bearing_diff) * 10\n",
    "    #조류 보상,패널티\n",
    "    tidal_dir, tidal_speed = get_tidal_effect(next_pos, tidal_data)\n",
    "    ship_dir = action * 45\n",
    "    tidal_angle_diff = abs(tidal_dir - ship_dir)\n",
    "    if tidal_angle_diff > 180:\n",
    "        tidal_angle_diff = 360 - tidal_angle_diff\n",
    "    tidal_reward = tidal_speed * np.cos(np.radians(tidal_angle_diff))\n",
    "    #지속적인 거리감소에대한 보상\n",
    "    current_distance = np.sqrt((current_pos[0] - goal[0])**2 + (current_pos[1] - goal[1])**2)\n",
    "    next_distance = np.sqrt((next_pos[0] - goal[0])**2 + (next_pos[1] - goal[1])**2)\n",
    "    distance_reward = (current_distance - next_distance) * 2\n",
    "    #기본이동 패널티\n",
    "    step_penalty = -0.1\n",
    "    #시간초과 패널티\n",
    "    timeout_penalty = -100 if step_count >= max_steps else 0\n",
    "    #탐색 유도 보상\n",
    "    exploration_bonus = 0.1 if random.random() < 0.1 else 0\n",
    "    #종합 보상계산\n",
    "    total_reward = (tidal_reward + bearing_reward + distance_reward + \n",
    "                    step_penalty + obstacle_penalty + direction_penalty + \n",
    "                    timeout_penalty + exploration_bonus)\n",
    "    return total_reward\n",
    "\n",
    "# Dueling DQN 모델 정의\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, num_actions=8):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(45, 512)\n",
    "        self.dropout1 = nn.Dropout(0.05)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.05)\n",
    "        self.value_stream = nn.Linear(256, 1)\n",
    "        self.advantage_stream = nn.Linear(256, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        value = self.value_stream(x)\n",
    "        advantages = self.advantage_stream(x)\n",
    "        q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# Prioritized Replay Buffer 정의\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity=50000, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.pos = 0\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(self.max_priority)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "            self.priorities[self.pos] = self.max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        # dones를 float로 명시적으로 변환\n",
    "        dones = torch.FloatTensor([float(d) for d in dones]).to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = min(abs(td_error) + 1e-5, self.max_priority)\n",
    "            self.max_priority = max(self.max_priority, self.priorities[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 학습 함수\n",
    "def train_dqn(episodes=TRAINING_EPISODES, batch_size=BATCH_SIZE, gamma=GAMMA, \n",
    "              epsilon_start=EPSILON_START, epsilon_end=EPSILON_END, \n",
    "              epsilon_decay=EPSILON_DECAY, save_interval=SAVE_INTERVAL, \n",
    "              tidal_base_path=TIDAL_BASE_PATH, load_model_path=LOAD_MODEL_PATH, \n",
    "              checkpoint_path=CHECKPOINT_PATH):\n",
    "    model = DuelingDQN().to(device)\n",
    "    target_model = DuelingDQN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    replay_buffer = PrioritizedReplayBuffer()\n",
    "    start_episode = 0\n",
    "    epsilon = epsilon_start\n",
    "    total_rewards = []\n",
    "    success_rates = []\n",
    "\n",
    "    # 모델 로드\n",
    "    if load_model_path and os.path.exists(load_model_path):\n",
    "        checkpoint = torch.load(load_model_path)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            target_model.load_state_dict(checkpoint['target_model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_episode = checkpoint['episode']\n",
    "            epsilon = checkpoint['epsilon']\n",
    "            replay_buffer.buffer = checkpoint['replay_buffer']\n",
    "            print(f\"{load_model_path}에서 체크포인트 로드. {start_episode} 에피소드부터 시작.\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            target_model.load_state_dict(checkpoint)\n",
    "            start_episode = 10000\n",
    "            epsilon = EPSILON_END\n",
    "            print(f\"{load_model_path}에서 가중치 로드. {start_episode} 에피소드부터 시작.\")\n",
    "\n",
    "    pbar = tqdm(range(start_episode, episodes), desc=\"Training Progress\")\n",
    "\n",
    "    for episode in pbar:\n",
    "        tidal_file = get_random_tidal_file(tidal_base_path)\n",
    "        tidal_data = load_tidal_data(tidal_file)\n",
    "        grid = create_grid_map()\n",
    "        start_latlon, goal_latlon = START_GOAL_COORDS[episode % 2]\n",
    "        start = latlon_to_grid(start_latlon[0], start_latlon[1])\n",
    "        goal = latlon_to_grid(goal_latlon[0], goal_latlon[1])\n",
    "        grid[start] = 0\n",
    "        grid[goal] = 0\n",
    "        \n",
    "        # 초기 상태 정의 (45차원)\n",
    "        tidal_dir, tidal_speed = get_tidal_effect(start, tidal_data)\n",
    "        tidal_dirs, tidal_speeds = get_adjacent_tidal_info(start, tidal_data)\n",
    "        obstacle_info = get_obstacle_info(start, grid)\n",
    "        bearing_diff = calculate_bearing_diff(0, goal, start)\n",
    "        state = [\n",
    "            start[0] / GRID_SIZE, start[1] / GRID_SIZE,\n",
    "            goal[0] / GRID_SIZE, goal[1] / GRID_SIZE,\n",
    "            bearing_diff,\n",
    "        ] + tidal_dirs + tidal_speeds + obstacle_info\n",
    "        \n",
    "        prev_action = None\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        success = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done and step_count < MAX_STEPS:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, 7)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with autocast('cuda'):\n",
    "                    q_values = model(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "            next_pos = move((int(state[0] * GRID_SIZE), int(state[1] * GRID_SIZE)), action)\n",
    "            reward = calculate_reward(grid, (int(state[0] * GRID_SIZE), int(state[1] * GRID_SIZE)), action, next_pos, prev_action, tidal_data, goal, step_count)\n",
    "            total_reward += reward\n",
    "            if next_pos == goal:\n",
    "                success = 1\n",
    "            \n",
    "            # 다음 상태 정의 (45차원)\n",
    "            tidal_dir, tidal_speed = get_tidal_effect(next_pos, tidal_data)\n",
    "            tidal_dirs, tidal_speeds = get_adjacent_tidal_info(next_pos, tidal_data)\n",
    "            obstacle_info = get_obstacle_info(next_pos, grid)\n",
    "            bearing_diff = calculate_bearing_diff(action, goal, next_pos)\n",
    "            next_state = [\n",
    "                next_pos[0] / GRID_SIZE, next_pos[1] / GRID_SIZE,\n",
    "                goal[0] / GRID_SIZE, goal[1] / GRID_SIZE,\n",
    "                bearing_diff,\n",
    "            ] + tidal_dirs + tidal_speeds + obstacle_info\n",
    "            \n",
    "            done = (next_pos == goal) or (step_count >= MAX_STEPS) or reward <= -200\n",
    "\n",
    "            replay_buffer.add((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            prev_action = action\n",
    "            step_count += 1\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones, indices, weights = replay_buffer.sample(batch_size)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast('cuda'):\n",
    "                    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                    with torch.no_grad():\n",
    "                        # Double DQN: 온라인 네트워크로 행동 선택, 타겟 네트워크로 Q값 평가\n",
    "                        next_actions = model(next_states).argmax(1, keepdim=True)\n",
    "                        next_q_values = target_model(next_states).gather(1, next_actions).squeeze(1)\n",
    "                        targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "                    td_errors = (q_values - targets).abs().cpu().detach().numpy()\n",
    "                    loss = (weights * (q_values - targets) ** 2).mean()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                replay_buffer.update_priorities(indices, td_errors)\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        if episode % 50 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        if episode % save_interval == 0 and episode > 0:\n",
    "            os.makedirs(\"C:/baramproject/trained_model/sibal/models\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"C:/baramproject/trained_model/sibal/models/dqn_model_{episode}.pth\")\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        success_rates.append(success)\n",
    "        pbar.set_postfix({\"Reward\": f\"{total_reward:.2f}\", \"Success\": success})\n",
    "\n",
    "    # 최종 모델 및 체크포인트 저장\n",
    "    torch.save(model.state_dict(), \"C:/baramproject/trained_model/sibal/models/dqn_model_final.pth\")\n",
    "    print(f\"최종 모델 저장: C:/baramproject/trained_model/sibal/models/dqn_model_final.pth\")\n",
    "    checkpoint = {\n",
    "        'episode': episodes,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'target_model_state_dict': target_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epsilon': epsilon,\n",
    "        'replay_buffer': replay_buffer.buffer\n",
    "    }\n",
    "    torch.save(checkpoint, CHECKPOINT_PATH)\n",
    "    print(f\"체크포인트 저장: {CHECKPOINT_PATH}\")\n",
    "\n",
    "    # 학습 진행 그래프\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_rewards, label=\"Total Reward\")\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Progress - Total Reward per Episode')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(success_rates, label=\"Success Rate\", color='green')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success (1=Yes, 0=No)')\n",
    "    plt.title('Training Progress - Goal Success Rate')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_dqn(load_model_path=LOAD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04653e-4db9-4364-9146-b05f876620ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
