{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b8e3e18-e7ae-4b9e-933d-bf3122d03764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8a76e68c114c3abfae170aeb47f3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 179\u001b[0m\n\u001b[0;32m    177\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    178\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m--> 179\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate_target_model()\n\u001b[0;32m    182\u001b[0m rewards_history\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "Cell \u001b[1;32mIn[23], line 138\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    135\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mminibatch)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# 리스트를 NumPy 배열로 변환 후 텐서로 변환\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    139\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(np\u001b[38;5;241m.\u001b[39marray(actions))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    140\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(rewards))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# CUDA 설정 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "\n",
    "# 1. 그리드 맵 로드 및 좌표 변환 함수 정의\n",
    "grid_map = np.load(r\"C:\\baramproject\\sibal\\land_sea_grid_cartopy.npy\")\n",
    "n_rows, n_cols = grid_map.shape  # 810x710\n",
    "\n",
    "# 위경도 범위\n",
    "lon_min, lon_max = 120.0, 127.0\n",
    "lat_min, lat_max = 30.0, 38.0\n",
    "\n",
    "def latlon_to_grid(lat, lon):\n",
    "    \"\"\"위경도를 그리드 픽셀 좌표로 변환\"\"\"\n",
    "    x = int((lon - lon_min) / (lon_max - lon_min) * (n_cols - 1))\n",
    "    y = int((lat_max - lat) / (lat_max - lat_min) * (n_rows - 1))\n",
    "    return y, x\n",
    "\n",
    "# 시작점과 도착점 설정 (인천항과 상하이항)\n",
    "start_point = latlon_to_grid(37.46036, 126.62360)  # 인천항\n",
    "goal_point = latlon_to_grid(30.62828, 122.06400)   # 상하이항\n",
    "\n",
    "# 2. 강화 학습 환경 정의\n",
    "class NavigationEnv:\n",
    "    def __init__(self, grid, start, goal):\n",
    "        self.grid = grid\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.agent_pos = start\n",
    "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1),  # 상, 하, 좌, 우\n",
    "                        (-1, -1), (-1, 1), (1, -1), (1, 1)]  # 대각선\n",
    "        self.max_steps = 1000  # 최대 스텝 제한\n",
    "\n",
    "    def reset(self, reverse=False):\n",
    "        \"\"\"환경 초기화, reverse=True면 목표와 시작점 반전\"\"\"\n",
    "        if reverse:\n",
    "            self.agent_pos = self.goal\n",
    "            self.goal_pos = self.start\n",
    "        else:\n",
    "            self.agent_pos = self.start\n",
    "            self.goal_pos = self.goal\n",
    "        self.steps = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"현재 상태 반환: 에이전트 위치와 목표 위치\"\"\"\n",
    "        return np.array([self.agent_pos[0], self.agent_pos[1], \n",
    "                         self.goal_pos[0], self.goal_pos[1]])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"행동 수행\"\"\"\n",
    "        self.steps += 1\n",
    "        new_pos = (self.agent_pos[0] + self.actions[action][0],\n",
    "                   self.agent_pos[1] + self.actions[action][1])\n",
    "\n",
    "        # 그리드 경계 및 육지 확인\n",
    "        if (0 <= new_pos[0] < n_rows and 0 <= new_pos[1] < n_cols and \n",
    "            self.grid[new_pos[0], new_pos[1]] == 0):  # 바다일 경우만 이동\n",
    "            self.agent_pos = new_pos\n",
    "            reward = self._calculate_reward()\n",
    "            done = self.agent_pos == self.goal_pos or self.steps >= self.max_steps\n",
    "            return self._get_state(), reward, done\n",
    "        else:  # 육지나 경계 밖\n",
    "            return self._get_state(), -10, False\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        \"\"\"보상 계산\"\"\"\n",
    "        dist = np.sqrt((self.agent_pos[0] - self.goal_pos[0])**2 + \n",
    "                       (self.agent_pos[1] - self.goal_pos[1])**2)\n",
    "        prev_dist = np.sqrt((self.agent_pos[0] + self.actions[-1][0] - self.goal_pos[0])**2 + \n",
    "                            (self.agent_pos[1] + self.actions[-1][1] - self.goal_pos[1])**2)\n",
    "        reward = (prev_dist - dist) * 0.1  # 거리 감소에 비례한 보상\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward += 100  # 목표 도달 시 큰 보상\n",
    "        reward -= 1  # 이동마다 작은 패널티 (최단 경로 유도)\n",
    "        return reward\n",
    "\n",
    "# 3. DQN 모델 정의\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 4. 에이전트 정의\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # 할인율\n",
    "        self.epsilon = 1.0  # 탐험률\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQN(state_size, action_size).to(device)\n",
    "        self.target_model = DQN(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        # 메모리에서 배치 샘플링\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        \n",
    "        # 리스트를 NumPy 배열로 변환 후 텐서로 변환\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(np.array(actions)).to(device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).to(device)\n",
    "\n",
    "        # Q값 계산\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        targets = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        # 손실 계산 및 학습\n",
    "        loss = nn.MSELoss()(q_values, targets.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 5. 학습 루프\n",
    "env = NavigationEnv(grid_map, start_point, goal_point)\n",
    "agent = DQNAgent(state_size=4, action_size=8)\n",
    "episodes = 500\n",
    "batch_size = 32\n",
    "rewards_history = []\n",
    "\n",
    "plt.ion()  # 실시간 플롯 활성화\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for e in tqdm(range(episodes), desc=\"Training Episodes\"):\n",
    "    state = env.reset(reverse=(e % 2 == 1))  # 왕복 설정\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        agent.replay(batch_size)\n",
    "    \n",
    "    agent.update_target_model()\n",
    "    rewards_history.append(total_reward)\n",
    "    \n",
    "    # 실시간 리워드 그래프 갱신\n",
    "    clear_output(wait=True)\n",
    "    ax.clear()\n",
    "    ax.plot(rewards_history, label='Total Reward')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.set_title('Training Progress')\n",
    "    ax.legend()\n",
    "    plt.pause(0.001)\n",
    "    tqdm.write(f\"Episode {e+1}/{episodes} - Reward: {total_reward:.2f} - Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(agent.model.state_dict(), r\"C:\\baramproject\\trained_model\\sibal7\\dqn_navigation_model.pth\")\n",
    "print(\"모델 저장 완료: dqn_navigation_model.pth\")\n",
    "\n",
    "# 6. 예측 함수\n",
    "def predict_path(agent, env, start, goal):\n",
    "    env.reset()\n",
    "    env.agent_pos = start\n",
    "    env.goal_pos = goal\n",
    "    state = env._get_state()\n",
    "    path = [start]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        state, reward, done = env.step(action)\n",
    "        path.append((env.agent_pos[0], env.agent_pos[1]))\n",
    "        if len(path) > env.max_steps:\n",
    "            break\n",
    "    \n",
    "    return path\n",
    "\n",
    "# 테스트: 인천항 -> 상하이항 경로 예측\n",
    "learned_path = predict_path(agent, env, start_point, goal_point)\n",
    "print(\"예측된 경로 (픽셀 좌표):\", learned_path)\n",
    "\n",
    "# 경로 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(grid_map, cmap='gray', origin='upper')\n",
    "path_y, path_x = zip(*learned_path)\n",
    "plt.plot(path_x, path_y, 'r.-', label='Predicted Path')\n",
    "plt.scatter([start_point[1], goal_point[1]], [start_point[0], goal_point[0]], \n",
    "            c='yellow', label='Start/Goal')\n",
    "plt.legend()\n",
    "plt.title('Optimal Navigation Path')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
