{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f905aaf7-ce5a-44a4-94a5-609fc28ffa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a657867fbc11439f9c9362b7c23c6280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -3066.5816721047627, Path Length: 300\n",
      "Episode 100, Total Reward: -521.3756744589826, Path Length: 300\n",
      "Episode 200, Total Reward: 16.15992522806465, Path Length: 300\n",
      "Episode 300, Total Reward: 132.76008370803322, Path Length: 300\n",
      "Episode 400, Total Reward: 42.73644971508658, Path Length: 300\n",
      "Episode 500, Total Reward: -22.95471225333901, Path Length: 300\n",
      "Episode 600, Total Reward: 25.44360584774398, Path Length: 300\n",
      "Episode 700, Total Reward: 39.06387280801141, Path Length: 300\n",
      "Episode 800, Total Reward: 92.96721129149414, Path Length: 300\n",
      "Episode 900, Total Reward: 108.64027623787109, Path Length: 300\n",
      "Episode 1000, Total Reward: 45.603643538426866, Path Length: 300\n",
      "Episode 1100, Total Reward: 39.387202245476146, Path Length: 300\n",
      "Episode 1200, Total Reward: 61.32925566435068, Path Length: 300\n",
      "Episode 1300, Total Reward: 26.757159385695957, Path Length: 300\n",
      "Episode 1400, Total Reward: 20.92972983588207, Path Length: 300\n",
      "Episode 1500, Total Reward: 67.85372107808519, Path Length: 300\n",
      "Episode 1600, Total Reward: 15.814137333130446, Path Length: 300\n",
      "Episode 1700, Total Reward: 10.452934292300846, Path Length: 300\n",
      "Episode 1800, Total Reward: 41.63385739766116, Path Length: 300\n",
      "Episode 1900, Total Reward: 42.436017398183424, Path Length: 300\n",
      "Episode 2000, Total Reward: 67.53094208406225, Path Length: 300\n",
      "Episode 2100, Total Reward: 47.79710208644727, Path Length: 300\n",
      "Episode 2200, Total Reward: 91.32648001958859, Path Length: 300\n",
      "Episode 2300, Total Reward: 86.16687164701119, Path Length: 300\n",
      "Episode 2400, Total Reward: 92.55797329584203, Path Length: 300\n",
      "Episode 2500, Total Reward: 56.592905171188335, Path Length: 300\n",
      "Episode 2600, Total Reward: 67.77880342454105, Path Length: 300\n",
      "Episode 2700, Total Reward: -23.111043433593977, Path Length: 300\n",
      "Episode 2800, Total Reward: 77.18099093680442, Path Length: 300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 484\u001b[0m\n\u001b[0;32m    481\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m    482\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim, action_dim)\n\u001b[1;32m--> 484\u001b[0m rewards, path_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n\u001b[0;32m    487\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Rewards Over Episodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 426\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(env, agent, max_episodes)\u001b[0m\n\u001b[0;32m    424\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(agent\u001b[38;5;241m.\u001b[39mepsilon_end, epsilon \u001b[38;5;241m-\u001b[39m (agent\u001b[38;5;241m.\u001b[39mepsilon_start \u001b[38;5;241m-\u001b[39m agent\u001b[38;5;241m.\u001b[39mepsilon_end) \u001b[38;5;241m/\u001b[39m agent\u001b[38;5;241m.\u001b[39mepsilon_decay)\n\u001b[0;32m    425\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state, epsilon)\n\u001b[1;32m--> 426\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m q_values \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy_net(torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    429\u001b[0m debug_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: path_length,\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: state\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m: epsilon\n\u001b[0;32m    437\u001b[0m })\n",
      "Cell \u001b[1;32mIn[2], line 257\u001b[0m, in \u001b[0;36mNavigationEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_time \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time \u001b[38;5;241m=\u001b[39m next_time\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_tidal_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_tidal_to_grid()\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_wind_data()\n",
      "Cell \u001b[1;32mIn[2], line 181\u001b[0m, in \u001b[0;36mNavigationEnv.load_tidal_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_tidal_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    180\u001b[0m     time_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m     tidal_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtidal_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtidal_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtidal_data \u001b[38;5;241m=\u001b[39m tidal_data \u001b[38;5;28;01mif\u001b[39;00m tidal_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 158\u001b[0m, in \u001b[0;36mNavigationEnv.load_data\u001b[1;34m(self, data_dir, filename_prefix, time_str)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 158\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import atan2, degrees, radians, cos, sin\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# CUDA 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 경험 저장을 위한 named tuple 정의\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# Dueling DQN 네트워크 정의\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # 상태 가치 스트림\n",
    "        self.value_stream = nn.Linear(64, 1)\n",
    "        # 액션 이점 스트림\n",
    "        self.advantage_stream = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Q 값 계산: V(s) + (A(s,a) - mean(A(s)))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# SumTree 클래스 정의 (PER을 위한 우선순위 관리)\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # 우선순위를 저장하는 트리\n",
    "        self.data = np.zeros(capacity, dtype=object)  # 경험 데이터를 저장\n",
    "        self.data_pointer = 0  # 다음 저장 위치\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, priority)\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0  # 버퍼가 가득 차면 처음부터 덮어씀\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        # 부모 노드 업데이트\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, s):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if s <= self.tree[left_child_idx]:\n",
    "                parent_idx = left_child_idx\n",
    "            else:\n",
    "                s -= self.tree[left_child_idx]\n",
    "                parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]  # 루트 노드의 합계\n",
    "\n",
    "# 항해 환경 클래스 정의 (변경 없음)\n",
    "class NavigationEnv:\n",
    "    def __init__(self):\n",
    "        self.grid = np.load('land_sea_grid_cartopy_downsized.npy')\n",
    "        self.n_rows, self.n_cols = self.grid.shape\n",
    "        self.lat_min, self.lat_max = 30, 38\n",
    "        self.lon_min, self.lon_max = 120, 127\n",
    "        self.start_pos = self.latlon_to_grid(37.46036, 126.52360)\n",
    "        self.end_pos = self.latlon_to_grid(30.62828, 122.06400)\n",
    "        self.step_time_minutes = 12\n",
    "        self.max_steps = 300\n",
    "        self.cumulative_time = 0\n",
    "        self.step_count = 0\n",
    "        self.tidal_data_dir = r\"C:\\baramproject\\tidal_database\"\n",
    "        self.wind_data_dir = r\"C:\\baramproject\\wind_database_2\"\n",
    "        self.action_space = np.array([0, 45, 90, 135, 180, -135, -90, -45])\n",
    "        self.grid_directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.k_c = 0.1\n",
    "        self.k_w = 0.005\n",
    "        self.path = []\n",
    "        self.reset()\n",
    "\n",
    "    def latlon_to_grid(self, lat, lon):\n",
    "        row = int((self.lat_max - lat) / (self.lat_max - self.lat_min) * self.n_rows)\n",
    "        col = int((lon - self.lon_min) / (self.lon_max - self.lon_min) * self.n_cols)\n",
    "        return row, col\n",
    "\n",
    "    def reset(self, start_time=None):\n",
    "        start_date = datetime(2018, 1, 1, 0, 0)\n",
    "        end_date = datetime(2018, 12, 29, 0, 0)\n",
    "        if start_time is None:\n",
    "            time_delta = (end_date - start_date).total_seconds()\n",
    "            random_seconds = np.random.randint(0, int(time_delta / 60 / 30) + 1) * 30 * 60\n",
    "            start_time = start_date + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        self.current_pos = self.start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.current_time = start_time\n",
    "        self.cumulative_time = 0\n",
    "        self.load_tidal_data()\n",
    "        self.map_tidal_to_grid()\n",
    "        self.load_wind_data()\n",
    "        self.map_wind_to_grid()\n",
    "        self.prev_distance = self.get_distance_to_end()\n",
    "        self.step_count = 0\n",
    "        self.path = [self.current_pos]\n",
    "        return self._get_state()\n",
    "\n",
    "    def get_relative_position_and_angle(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        distance = np.linalg.norm(rel_pos)\n",
    "        end_angle = degrees(atan2(rel_pos[1], rel_pos[0])) % 360\n",
    "        return rel_pos, distance, end_angle\n",
    "\n",
    "    def get_distance_to_end(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        return np.linalg.norm(rel_pos)\n",
    "\n",
    "    def angle_to_grid_direction(self, abs_action_angle):\n",
    "        grid_angles = np.array([0, 45, 90, 135, 180, 225, 270, 315])\n",
    "        angle_diff = np.abs(grid_angles - abs_action_angle)\n",
    "        closest_idx = np.argmin(angle_diff)\n",
    "        return self.grid_directions[closest_idx]\n",
    "\n",
    "    def load_data(self, data_dir, filename_prefix, time_str):\n",
    "        data_file = os.path.join(data_dir, f\"{filename_prefix}{time_str}.json\")\n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Warning: Data file {data_file} not found. Episode will be terminated.\")\n",
    "            return None\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data[\"result\"][\"data\"]\n",
    "\n",
    "    def map_data_to_grid(self, data, dir_key, speed_key):\n",
    "        grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "        if data is None:\n",
    "            return grid_dir, grid_speed, grid_valid\n",
    "        positions = [(float(item[\"pre_lat\"]), float(item[\"pre_lon\"])) for item in data]\n",
    "        directions = [float(item[dir_key]) for item in data]\n",
    "        speeds = [float(item[speed_key]) for item in data]\n",
    "        for pos, dir, speed in zip(positions, directions, speeds):\n",
    "            lat, lon = pos\n",
    "            row, col = self.latlon_to_grid(lat, lon)\n",
    "            if 0 <= row < self.n_rows and 0 <= col < self.n_cols:\n",
    "                grid_dir[row, col] = dir\n",
    "                grid_speed[row, col] = speed\n",
    "                grid_valid[row, col] = True\n",
    "        return grid_dir, grid_speed, grid_valid\n",
    "\n",
    "    def load_tidal_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        tidal_data = self.load_data(self.tidal_data_dir, \"tidal_\", time_str)\n",
    "        self.tidal_data = tidal_data if tidal_data is not None else None\n",
    "\n",
    "    def map_tidal_to_grid(self):\n",
    "        if self.tidal_data is not None:\n",
    "            self.tidal_grid_dir, self.tidal_grid_speed, self.tidal_grid_valid = self.map_data_to_grid(\n",
    "                self.tidal_data, \"current_dir\", \"current_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.tidal_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def load_wind_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        wind_data = self.load_data(self.wind_data_dir, \"wind_\", time_str)\n",
    "        self.wind_data = wind_data if wind_data is not None else None\n",
    "\n",
    "    def map_wind_to_grid(self):\n",
    "        if self.wind_data is not None:\n",
    "            self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid = self.map_data_to_grid(\n",
    "                self.wind_data, \"wind_dir\", \"wind_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.wind_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def calculate_fuel_consumption(self, abs_action_angle, position):\n",
    "        row, col = position\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        tidal_dir_rad = (90 - tidal_dir) * np.pi / 180\n",
    "        wind_dir_rad = (90 - wind_dir) * np.pi / 180\n",
    "        action_angle_rad = (90 - abs_action_angle) * np.pi / 180\n",
    "        theta_c = action_angle_rad - tidal_dir_rad\n",
    "        theta_w = action_angle_rad - wind_dir_rad\n",
    "        f_0 = 1\n",
    "        tidal_effect = -self.k_c * tidal_speed * cos(theta_c)\n",
    "        wind_effect = self.k_w * wind_speed * cos(theta_w)\n",
    "        total_fuel = f_0 + wind_effect + tidal_effect\n",
    "        return total_fuel\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        rel_action_angle = self.action_space[action]\n",
    "        abs_action_angle = (end_angle + rel_action_angle) % 360\n",
    "        turn_penalty = 0\n",
    "        if hasattr(self, 'previous_direction') and self.previous_direction is not None:\n",
    "            angle_diff = min((abs_action_angle - self.previous_direction) % 360, \n",
    "                             (self.previous_direction - abs_action_angle) % 360)\n",
    "            turn_penalty = angle_diff * 0.1\n",
    "        move_dir = self.angle_to_grid_direction(abs_action_angle)\n",
    "        new_pos = (self.current_pos[0] + move_dir[0], self.current_pos[1] + move_dir[1])\n",
    "        current_fuel = self.calculate_fuel_consumption(abs_action_angle, self.current_pos)\n",
    "        next_fuel = self.calculate_fuel_consumption(abs_action_angle, new_pos)\n",
    "        fuel_reduction = current_fuel - next_fuel\n",
    "        if (0 <= new_pos[0] < self.n_rows and 0 <= new_pos[1] < self.n_cols and \n",
    "            self.grid[new_pos[0], new_pos[1]] == 0):\n",
    "            self.current_pos = new_pos\n",
    "            self.path.append(self.current_pos)\n",
    "        self.previous_direction = abs_action_angle\n",
    "        self.prev_action = action\n",
    "        self.cumulative_time += self.step_time_minutes\n",
    "        if self.cumulative_time >= 30:\n",
    "            next_time = self.current_time + timedelta(minutes=30)\n",
    "            end_date = datetime(2018, 12, 31, 23, 30)\n",
    "            if next_time <= end_date:\n",
    "                self.current_time = next_time\n",
    "                self.load_tidal_data()\n",
    "                self.map_tidal_to_grid()\n",
    "                self.load_wind_data()\n",
    "                self.map_wind_to_grid()\n",
    "            else:\n",
    "                print(\"Warning: Time exceeds 2018 range. Keeping previous data.\")\n",
    "            self.cumulative_time -= 30\n",
    "        state = self._get_state()\n",
    "        current_distance = self.get_distance_to_end()\n",
    "        distance_reward = (self.prev_distance - current_distance) * 2.0\n",
    "        \n",
    "        self.prev_distance = current_distance\n",
    "        goal_reward = 100 if tuple(self.current_pos) == self.end_pos else 0\n",
    "        reward = -current_fuel + fuel_reduction * 1.0 + distance_reward - turn_penalty + goal_reward\n",
    "        done = tuple(self.current_pos) == self.end_pos or self.step_count >= self.max_steps\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        row, col = self.current_pos\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if hasattr(self, 'tidal_grid_valid') and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if hasattr(self, 'wind_grid_valid') and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        return np.array([rel_pos[0], rel_pos[1], distance, tidal_dir, tidal_speed, wind_dir, wind_speed])\n",
    "\n",
    "# DQN 에이전트 클래스 정의 (PER 통합)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # 기존 하이퍼파라미터\n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.buffer_size = 100000\n",
    "        self.target_update = 1000\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 10000\n",
    "        \n",
    "        # PER 관련 하이퍼파라미터\n",
    "        self.alpha = 0.6  # 우선순위 가중치 조절\n",
    "        self.beta_start = 0.4  # 초기 샘플링 편향 보정\n",
    "        self.beta_end = 1.0  # 최종 샘플링 편향 보정\n",
    "        \n",
    "        # 신경망 및 옵티마이저\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        \n",
    "        # PER을 위한 SumTree 메모리\n",
    "        self.memory = SumTree(self.buffer_size)\n",
    "        self.step_count = 0\n",
    "        self.max_priority = 1.0  # 초기 최대 우선순위\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        self.step_count += 1\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        # 새로운 경험을 최대 우선순위로 저장\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.add(self.max_priority, experience)\n",
    "\n",
    "    def sample_batch(self, beta):\n",
    "        # 우선순위에 비례하여 경험 샘플링\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        priorities = []\n",
    "        segment = self.memory.total_priority() / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.memory.get_leaf(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        \n",
    "        # 샘플링 확률과 가중치 계산\n",
    "        sampling_probabilities = np.array(priorities) / self.memory.total_priority()\n",
    "        is_weight = np.power(self.buffer_size * sampling_probabilities, -beta)\n",
    "        is_weight /= is_weight.max()  # 정규화\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def compute_loss(self, batch, idxs, is_weight, beta):\n",
    "        # 배치에서 데이터 추출\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        # Q 값 계산\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(dim=1)[0]\n",
    "            targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        # TD 오차 계산 및 우선순위 업데이트\n",
    "        td_errors = torch.abs(targets - q_values)\n",
    "        loss = (torch.FloatTensor(is_weight).to(device) * (q_values - targets.detach()) ** 2).mean()\n",
    "        \n",
    "        for idx, td_error in zip(idxs, td_errors.detach().cpu().numpy()):\n",
    "            priority = td_error.item() ** self.alpha\n",
    "            self.memory.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        if self.memory.data_pointer < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Beta 값을 학습 진행에 따라 증가\n",
    "        beta = self.beta_start + (self.beta_end - self.beta_start) * min(1.0, self.step_count / 50000)\n",
    "        batch, idxs, is_weight = self.sample_batch(beta)\n",
    "        loss = self.compute_loss(batch, idxs, is_weight, beta)\n",
    "        \n",
    "        # 모델 업데이트\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# 학습 루프 정의 (변경 없음)\n",
    "def train_dqn(env, agent, max_episodes=30000):\n",
    "    rewards = []\n",
    "    path_lengths = []\n",
    "    epsilon = agent.epsilon_start\n",
    "    \n",
    "    image_dir = r\"C:\\baramproject\\trained_model\\sibal22\\episode_debug_image\"\n",
    "    data_dir = r\"C:\\baramproject\\trained_model\\sibal22\\episode_debug_data\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for episode in tqdm(range(max_episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path_length = 0\n",
    "        done = False\n",
    "        debug_data = []\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = max(agent.epsilon_end, epsilon - (agent.epsilon_start - agent.epsilon_end) / agent.epsilon_decay)\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            q_values = agent.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\n",
    "            debug_data.append({\n",
    "                \"step\": path_length,\n",
    "                \"state\": state.tolist(),\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"next_state\": next_state.tolist(),\n",
    "                \"q_values\": q_values.tolist(),\n",
    "                \"epsilon\": epsilon\n",
    "            })\n",
    "            \n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path_length += 1\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        path_lengths.append(path_length)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Path Length: {path_length}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(env.grid, cmap='gray')\n",
    "            path_array = np.array(env.path)\n",
    "            plt.plot(path_array[:, 1], path_array[:, 0], 'r-', label='Path')\n",
    "            plt.plot(env.start_pos[1], env.start_pos[0], 'go', label='Start')\n",
    "            plt.plot(env.end_pos[1], env.end_pos[0], 'bo', label='End')\n",
    "            plt.legend()\n",
    "            plt.title(f\"Episode {episode} Path\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            with open(os.path.join(data_dir, f\"episode_{episode}.json\"), 'w') as f:\n",
    "                json.dump(debug_data, f, indent=4)\n",
    "        \n",
    "        if episode % 10000 == 0 and episode > 0:\n",
    "            plt.plot(rewards)\n",
    "            plt.title(\"Total Rewards Over Episodes\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"rewards_episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), r\"C:\\baramproject\\trained_model\\sibal22\\navigation_model.pth\")\n",
    "    return rewards, path_lengths\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    env = NavigationEnv()\n",
    "    state_dim = 7\n",
    "    action_dim = len(env.action_space)\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    rewards, path_lengths = train_dqn(env, agent)\n",
    "    \n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Total Rewards Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(path_lengths)\n",
    "    plt.title(\"Path Lengths Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Path Length\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
