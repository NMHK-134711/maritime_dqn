{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706842ba-80b8-4534-9b8e-4e2a5b283aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3cfa3c790f4ef4ba8638f338b9f1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -2986.4437117947414, Path Length: 300\n",
      "Episode 100, Total Reward: 50.24923534367514, Path Length: 300\n",
      "Episode 200, Total Reward: 56.36311988241454, Path Length: 300\n",
      "Episode 300, Total Reward: -99.98181045900964, Path Length: 300\n",
      "Episode 400, Total Reward: 33.6682588950337, Path Length: 300\n",
      "Episode 500, Total Reward: 23.475417281498835, Path Length: 300\n",
      "Episode 600, Total Reward: 89.72438729615538, Path Length: 300\n",
      "Episode 700, Total Reward: 99.89141052223437, Path Length: 300\n",
      "Episode 800, Total Reward: 104.14880655128542, Path Length: 300\n",
      "Episode 900, Total Reward: 67.67593882809999, Path Length: 300\n",
      "Episode 1000, Total Reward: 55.339354088657416, Path Length: 300\n",
      "Episode 1100, Total Reward: 71.96192584822798, Path Length: 300\n",
      "Episode 1200, Total Reward: 20.28777167224107, Path Length: 300\n",
      "Episode 1300, Total Reward: 52.65108762528739, Path Length: 300\n",
      "Episode 1400, Total Reward: 115.58328104379967, Path Length: 300\n",
      "Episode 1500, Total Reward: -103.53979959783199, Path Length: 300\n",
      "Episode 1600, Total Reward: 88.44912729179327, Path Length: 300\n",
      "Episode 1700, Total Reward: 139.7337074795576, Path Length: 300\n",
      "Episode 1800, Total Reward: 107.3562289334877, Path Length: 300\n",
      "Episode 1900, Total Reward: 1.925422115434273, Path Length: 300\n",
      "Episode 2000, Total Reward: -2.381969176803736, Path Length: 300\n",
      "Episode 2100, Total Reward: 126.30423512943625, Path Length: 300\n",
      "Episode 2200, Total Reward: -14.298326946001772, Path Length: 300\n",
      "Episode 2300, Total Reward: -9.968122130285128, Path Length: 300\n",
      "Episode 2400, Total Reward: 2.7090536802672744, Path Length: 300\n",
      "Episode 2500, Total Reward: 74.57866357898973, Path Length: 300\n",
      "Episode 2600, Total Reward: 98.77754776659404, Path Length: 300\n",
      "Episode 2700, Total Reward: 105.1072822372231, Path Length: 300\n",
      "Episode 2800, Total Reward: 63.27893063669612, Path Length: 300\n",
      "Episode 2900, Total Reward: -49.57537855787061, Path Length: 300\n",
      "Episode 3000, Total Reward: 71.08797219971149, Path Length: 300\n",
      "Episode 3100, Total Reward: 69.78010477387633, Path Length: 300\n",
      "Episode 3200, Total Reward: -9.469596625522357, Path Length: 300\n",
      "Episode 3300, Total Reward: 95.19565407346713, Path Length: 300\n",
      "Episode 3400, Total Reward: -5.3502639965041965, Path Length: 300\n",
      "Episode 3500, Total Reward: 18.94730338243444, Path Length: 300\n",
      "Episode 3600, Total Reward: 34.83130314078613, Path Length: 300\n",
      "Episode 3700, Total Reward: 33.95021654669259, Path Length: 300\n",
      "Episode 3800, Total Reward: -113.57794396359846, Path Length: 300\n",
      "Episode 3900, Total Reward: -48.4898821683815, Path Length: 300\n",
      "Episode 4000, Total Reward: -17.698119603109348, Path Length: 300\n",
      "Episode 4100, Total Reward: -18.92563261133146, Path Length: 300\n",
      "Episode 4200, Total Reward: -109.15714003105936, Path Length: 300\n",
      "Episode 4300, Total Reward: -28.926898799125325, Path Length: 300\n",
      "Episode 4400, Total Reward: -22.652061488007398, Path Length: 300\n",
      "Episode 4500, Total Reward: -68.61439119140941, Path Length: 300\n",
      "Episode 4600, Total Reward: -2.9060580606252415, Path Length: 300\n",
      "Episode 4700, Total Reward: 23.490763109748457, Path Length: 300\n",
      "Episode 4800, Total Reward: -42.78610509408615, Path Length: 300\n",
      "Episode 4900, Total Reward: 5.064769955762642, Path Length: 300\n",
      "Episode 5000, Total Reward: 10.260295998157574, Path Length: 300\n",
      "Episode 5100, Total Reward: -35.85110976616764, Path Length: 300\n",
      "Episode 5200, Total Reward: 34.20544981700451, Path Length: 300\n",
      "Episode 5300, Total Reward: -26.37872839240582, Path Length: 300\n",
      "Episode 5400, Total Reward: -6.2049265831386435, Path Length: 300\n",
      "Episode 5500, Total Reward: -100.02156111218636, Path Length: 300\n",
      "Episode 5600, Total Reward: -104.78486488908324, Path Length: 300\n",
      "Episode 5700, Total Reward: -21.43300573458413, Path Length: 300\n",
      "Episode 5800, Total Reward: -7.579047590846329, Path Length: 300\n",
      "Episode 5900, Total Reward: -86.53527036814504, Path Length: 300\n",
      "Episode 6000, Total Reward: 21.620619806374975, Path Length: 300\n",
      "Episode 6100, Total Reward: -27.54027272019512, Path Length: 300\n",
      "Episode 6200, Total Reward: -63.27206553367217, Path Length: 300\n",
      "Episode 6300, Total Reward: 27.08459692941537, Path Length: 300\n",
      "Episode 6400, Total Reward: -14.685067957923607, Path Length: 300\n",
      "Episode 6500, Total Reward: 43.870919656963395, Path Length: 300\n",
      "Episode 6600, Total Reward: 66.65321018430032, Path Length: 300\n",
      "Episode 6700, Total Reward: 32.40100594241329, Path Length: 300\n",
      "Episode 6800, Total Reward: -15.07308042368767, Path Length: 300\n",
      "Episode 6900, Total Reward: 43.13455282624258, Path Length: 300\n",
      "Episode 7000, Total Reward: 25.395034362735885, Path Length: 300\n",
      "Episode 7100, Total Reward: 56.48849756895575, Path Length: 300\n",
      "Episode 7200, Total Reward: 3.261145424079075, Path Length: 300\n",
      "Episode 7300, Total Reward: 85.15737864695438, Path Length: 300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import atan2, degrees, radians, cos, sin\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# CUDA 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 경험 저장을 위한 named tuple 정의\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "# Dueling DQN 네트워크 정의\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # 상태 가치 스트림\n",
    "        self.value_stream = nn.Linear(64, 1)\n",
    "        # 액션 이점 스트림\n",
    "        self.advantage_stream = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Q 값 계산: V(s) + (A(s,a) - mean(A(s)))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# 항해 환경 클래스 정의 (변경 없음)\n",
    "class NavigationEnv:\n",
    "    def __init__(self):\n",
    "        self.grid = np.load('land_sea_grid_cartopy_downsized.npy')\n",
    "        self.n_rows, self.n_cols = self.grid.shape\n",
    "        self.lat_min, self.lat_max = 30, 38\n",
    "        self.lon_min, self.lon_max = 120, 127\n",
    "        self.start_pos = self.latlon_to_grid(37.46036, 126.52360)\n",
    "        self.end_pos = self.latlon_to_grid(30.62828, 122.06400)\n",
    "        self.step_time_minutes = 12\n",
    "        self.max_steps = 300\n",
    "        self.cumulative_time = 0\n",
    "        self.step_count = 0\n",
    "        self.tidal_data_dir = r\"C:\\baramproject\\tidal_database\"\n",
    "        self.wind_data_dir = r\"C:\\baramproject\\wind_database_2\"\n",
    "        self.action_space = np.array([0, 45, 90, 135, 180, -135, -90, -45])\n",
    "        self.grid_directions = [(-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        self.k_c = 0.1\n",
    "        self.k_w = 0.005\n",
    "        self.path = []\n",
    "        self.reset()\n",
    "\n",
    "    def latlon_to_grid(self, lat, lon):\n",
    "        row = int((self.lat_max - lat) / (self.lat_max - self.lat_min) * self.n_rows)\n",
    "        col = int((lon - self.lon_min) / (self.lon_max - self.lon_min) * self.n_cols)\n",
    "        return row, col\n",
    "\n",
    "    def reset(self, start_time=None):\n",
    "        start_date = datetime(2018, 1, 1, 0, 0)\n",
    "        end_date = datetime(2018, 12, 29, 0, 0)\n",
    "        if start_time is None:\n",
    "            time_delta = (end_date - start_date).total_seconds()\n",
    "            random_seconds = np.random.randint(0, int(time_delta / 60 / 30) + 1) * 30 * 60\n",
    "            start_time = start_date + timedelta(seconds=random_seconds)\n",
    "        \n",
    "        self.current_pos = self.start_pos\n",
    "        self.visit_count = {}\n",
    "        self.prev_action = None\n",
    "        self.current_time = start_time\n",
    "        self.cumulative_time = 0\n",
    "        self.load_tidal_data()\n",
    "        self.map_tidal_to_grid()\n",
    "        self.load_wind_data()\n",
    "        self.map_wind_to_grid()\n",
    "        self.prev_distance = self.get_distance_to_end()\n",
    "        self.step_count = 0\n",
    "        self.path = [self.current_pos]\n",
    "        return self._get_state()\n",
    "\n",
    "    def get_relative_position_and_angle(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        distance = np.linalg.norm(rel_pos)\n",
    "        end_angle = degrees(atan2(rel_pos[1], rel_pos[0])) % 360\n",
    "        return rel_pos, distance, end_angle\n",
    "\n",
    "    def get_distance_to_end(self):\n",
    "        rel_pos = np.array(self.end_pos) - np.array(self.current_pos)\n",
    "        return np.linalg.norm(rel_pos)\n",
    "\n",
    "    def angle_to_grid_direction(self, abs_action_angle):\n",
    "        grid_angles = np.array([0, 45, 90, 135, 180, 225, 270, 315])\n",
    "        angle_diff = np.abs(grid_angles - abs_action_angle)\n",
    "        closest_idx = np.argmin(angle_diff)\n",
    "        return self.grid_directions[closest_idx]\n",
    "\n",
    "    def load_data(self, data_dir, filename_prefix, time_str):\n",
    "        data_file = os.path.join(data_dir, f\"{filename_prefix}{time_str}.json\")\n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Warning: Data file {data_file} not found. Episode will be terminated.\")\n",
    "            return None\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data[\"result\"][\"data\"]\n",
    "\n",
    "    def map_data_to_grid(self, data, dir_key, speed_key):\n",
    "        grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "        grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "        if data is None:\n",
    "            return grid_dir, grid_speed, grid_valid\n",
    "        positions = [(float(item[\"pre_lat\"]), float(item[\"pre_lon\"])) for item in data]\n",
    "        directions = [float(item[dir_key]) for item in data]\n",
    "        speeds = [float(item[speed_key]) for item in data]\n",
    "        for pos, dir, speed in zip(positions, directions, speeds):\n",
    "            lat, lon = pos\n",
    "            row, col = self.latlon_to_grid(lat, lon)\n",
    "            if 0 <= row < self.n_rows and 0 <= col < self.n_cols:\n",
    "                grid_dir[row, col] = dir\n",
    "                grid_speed[row, col] = speed\n",
    "                grid_valid[row, col] = True\n",
    "        return grid_dir, grid_speed, grid_valid\n",
    "\n",
    "    def load_tidal_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        tidal_data = self.load_data(self.tidal_data_dir, \"tidal_\", time_str)\n",
    "        self.tidal_data = tidal_data if tidal_data is not None else None\n",
    "\n",
    "    def map_tidal_to_grid(self):\n",
    "        if self.tidal_data is not None:\n",
    "            self.tidal_grid_dir, self.tidal_grid_speed, self.tidal_grid_valid = self.map_data_to_grid(\n",
    "                self.tidal_data, \"current_dir\", \"current_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.tidal_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.tidal_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def load_wind_data(self):\n",
    "        time_str = self.current_time.strftime(\"%Y%m%d_%H%M\")\n",
    "        wind_data = self.load_data(self.wind_data_dir, \"wind_\", time_str)\n",
    "        self.wind_data = wind_data if wind_data is not None else None\n",
    "\n",
    "    def map_wind_to_grid(self):\n",
    "        if self.wind_data is not None:\n",
    "            self.wind_grid_dir, self.wind_grid_speed, self.wind_grid_valid = self.map_data_to_grid(\n",
    "                self.wind_data, \"wind_dir\", \"wind_speed\"\n",
    "            )\n",
    "        else:\n",
    "            self.wind_grid_dir = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_speed = np.zeros((self.n_rows, self.n_cols))\n",
    "            self.wind_grid_valid = np.zeros((self.n_rows, self.n_cols), dtype=bool)\n",
    "\n",
    "    def calculate_fuel_consumption(self, abs_action_angle, position):\n",
    "        row, col = position\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if 0 <= row < self.n_rows and 0 <= col < self.n_cols and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        tidal_dir_rad = (90 - tidal_dir) * np.pi / 180\n",
    "        wind_dir_rad = (90 - wind_dir) * np.pi / 180\n",
    "        action_angle_rad = (90 - abs_action_angle) * np.pi / 180\n",
    "        theta_c = action_angle_rad - tidal_dir_rad\n",
    "        theta_w = action_angle_rad - wind_dir_rad\n",
    "        f_0 = 1\n",
    "        tidal_effect = -self.k_c * tidal_speed * cos(theta_c)\n",
    "        wind_effect = self.k_w * wind_speed * cos(theta_w)\n",
    "        total_fuel = f_0 + wind_effect + tidal_effect\n",
    "        return total_fuel\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        rel_action_angle = self.action_space[action]\n",
    "        abs_action_angle = (end_angle + rel_action_angle) % 360\n",
    "        turn_penalty = 0\n",
    "        if hasattr(self, 'previous_direction') and self.previous_direction is not None:\n",
    "            angle_diff = min((abs_action_angle - self.previous_direction) % 360, \n",
    "                             (self.previous_direction - abs_action_angle) % 360)\n",
    "            turn_penalty = angle_diff * 0.1\n",
    "        move_dir = self.angle_to_grid_direction(abs_action_angle)\n",
    "        new_pos = (self.current_pos[0] + move_dir[0], self.current_pos[1] + move_dir[1])\n",
    "        current_fuel = self.calculate_fuel_consumption(abs_action_angle, self.current_pos)\n",
    "        next_fuel = self.calculate_fuel_consumption(abs_action_angle, new_pos)\n",
    "        fuel_reduction = current_fuel - next_fuel\n",
    "        if (0 <= new_pos[0] < self.n_rows and 0 <= new_pos[1] < self.n_cols and \n",
    "            self.grid[new_pos[0], new_pos[1]] == 0):\n",
    "            self.current_pos = new_pos\n",
    "            self.path.append(self.current_pos)\n",
    "        self.previous_direction = abs_action_angle\n",
    "        self.prev_action = action\n",
    "        self.cumulative_time += self.step_time_minutes\n",
    "        if self.cumulative_time >= 30:\n",
    "            next_time = self.current_time + timedelta(minutes=30)\n",
    "            end_date = datetime(2018, 12, 31, 23, 30)\n",
    "            if next_time <= end_date:\n",
    "                self.current_time = next_time\n",
    "                self.load_tidal_data()\n",
    "                self.map_tidal_to_grid()\n",
    "                self.load_wind_data()\n",
    "                self.map_wind_to_grid()\n",
    "            else:\n",
    "                print(\"Warning: Time exceeds 2018 range. Keeping previous data.\")\n",
    "            self.cumulative_time -= 30\n",
    "        state = self._get_state()\n",
    "        current_distance = self.get_distance_to_end()\n",
    "        distance_reward = (self.prev_distance - current_distance) * 2.0\n",
    "        \n",
    "        self.prev_distance = current_distance\n",
    "        goal_reward = 100 if tuple(self.current_pos) == self.end_pos else 0\n",
    "        reward = -current_fuel + fuel_reduction * 1.0 + distance_reward - turn_penalty + goal_reward\n",
    "        done = tuple(self.current_pos) == self.end_pos or self.step_count >= self.max_steps\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        row, col = self.current_pos\n",
    "        rel_pos, distance, end_angle = self.get_relative_position_and_angle()\n",
    "        tidal_dir, tidal_speed = 0, 0\n",
    "        if hasattr(self, 'tidal_grid_valid') and self.tidal_grid_valid[row, col]:\n",
    "            tidal_dir = self.tidal_grid_dir[row, col]\n",
    "            tidal_speed = self.tidal_grid_speed[row, col]\n",
    "        wind_dir, wind_speed = 0, 0\n",
    "        if hasattr(self, 'wind_grid_valid') and self.wind_grid_valid[row, col]:\n",
    "            wind_dir = self.wind_grid_dir[row, col]\n",
    "            wind_speed = self.wind_grid_speed[row, col]\n",
    "        return np.array([rel_pos[0], rel_pos[1], distance, tidal_dir, tidal_speed, wind_dir, wind_speed])\n",
    "\n",
    "# DQN 에이전트 클래스 정의 (Multi-Step Learning 추가)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.buffer_size = 100000\n",
    "        self.target_update = 1000\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 10000\n",
    "        self.n_steps = 3  # Multi-step learning의 스텝 수\n",
    "        \n",
    "        self.alpha = 0.6\n",
    "        self.beta_start = 0.4\n",
    "        self.beta_end = 1.0\n",
    "        \n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.memory = deque(maxlen=self.buffer_size)\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        self.step_count += 1\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append((experience, 1.0))  # 우선순위는 현재 1.0으로 고정\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"배치 샘플링: n-step 학습을 위해 인덱스를 샘플링\"\"\"\n",
    "        max_idx = len(self.memory) - self.n_steps\n",
    "        if max_idx < 0:  # 메모리가 충분히 차지 않았을 경우\n",
    "            return []\n",
    "        # max_idx + 1까지 샘플링하여 최소 n_steps 이후의 전이를 확보\n",
    "        indices = random.sample(range(max_idx + 1), min(self.batch_size, max_idx + 1))\n",
    "        return indices\n",
    "\n",
    "    def compute_loss(self, batch_indices, beta):\n",
    "        if not batch_indices:\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in batch_indices:\n",
    "            exp = self.memory[i][0]\n",
    "            state = exp.state\n",
    "            action = exp.action\n",
    "            target = 0.0\n",
    "            g = 1.0\n",
    "            for m in range(self.n_steps):\n",
    "                if i + m >= len(self.memory):\n",
    "                    break\n",
    "                next_exp = self.memory[i + m][0]\n",
    "                target += g * next_exp.reward\n",
    "                if next_exp.done:\n",
    "                    break\n",
    "                g *= self.gamma\n",
    "            else:\n",
    "                # n-step이 끝나지 않았을 때 최대 Q 값을 추가\n",
    "                next_state = self.memory[i + self.n_steps - 1][0].next_state\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)  # 배치 차원 추가\n",
    "                with torch.no_grad():\n",
    "                    max_q = self.target_net(next_state_tensor).max(dim=1)[0].item()  # 행동 차원에서 최대값\n",
    "                target += g * self.gamma * max_q\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            targets.append(target)\n",
    "        \n",
    "        # 텐서로 변환\n",
    "        states = np.array(states)\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.array(actions)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        targets = np.array(targets)\n",
    "        targets = torch.from_numpy(targets).float().to(device)\n",
    "        \n",
    "        # Q 값 계산\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = nn.MSELoss()(q_values, targets.detach())\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"모델 업데이트\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        beta = self.beta_start + (self.beta_end - self.beta_start) * min(1.0, self.step_count / 50000)\n",
    "        batch_indices = self.sample_batch()\n",
    "        loss = self.compute_loss(batch_indices, beta)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# 학습 루프 정의 (변경 없음)\n",
    "def train_dqn(env, agent, max_episodes=20000):\n",
    "    rewards = []\n",
    "    path_lengths = []\n",
    "    epsilon = agent.epsilon_start\n",
    "    \n",
    "    image_dir = r\"C:\\baramproject\\trained_model\\sibal20\\episode_debug_image\"\n",
    "    data_dir = r\"C:\\baramproject\\trained_model\\sibal20\\episode_debug_data\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    for episode in tqdm(range(max_episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path_length = 0\n",
    "        done = False\n",
    "        debug_data = []\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = max(agent.epsilon_end, epsilon - (agent.epsilon_start - agent.epsilon_end) / agent.epsilon_decay)\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            q_values = agent.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\n",
    "            debug_data.append({\n",
    "                \"step\": path_length,\n",
    "                \"state\": state.tolist(),\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"next_state\": next_state.tolist(),\n",
    "                \"q_values\": q_values.tolist(),\n",
    "                \"epsilon\": epsilon\n",
    "            })\n",
    "            \n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path_length += 1\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        path_lengths.append(path_length)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Path Length: {path_length}\")\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(env.grid, cmap='gray')\n",
    "            path_array = np.array(env.path)\n",
    "            plt.plot(path_array[:, 1], path_array[:, 0], 'r-', label='Path')\n",
    "            plt.plot(env.start_pos[1], env.start_pos[0], 'go', label='Start')\n",
    "            plt.plot(env.end_pos[1], env.end_pos[0], 'bo', label='End')\n",
    "            plt.legend()\n",
    "            plt.title(f\"Episode {episode} Path\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            with open(os.path.join(data_dir, f\"episode_{episode}.json\"), 'w') as f:\n",
    "                json.dump(debug_data, f, indent=4)\n",
    "        \n",
    "        if episode % 1000 == 0 and episode > 0:\n",
    "            plt.plot(rewards)\n",
    "            plt.title(\"Total Rewards Over Episodes\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.savefig(os.path.join(image_dir, f\"rewards_episode_{episode}.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), r\"C:\\baramproject\\trained_model\\sibal20\\navigation_model.pth\")\n",
    "    return rewards, path_lengths\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    env = NavigationEnv()\n",
    "    state_dim = 7\n",
    "    action_dim = len(env.action_space)\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    rewards, path_lengths = train_dqn(env, agent)\n",
    "    \n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Total Rewards Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(path_lengths)\n",
    "    plt.title(\"Path Lengths Over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Path Length\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
